package ibis.satin;

import ibis.ipl.Ibis;
import ibis.ipl.IbisError;
import ibis.ipl.IbisException;
import ibis.ipl.IbisIdentifier;
import ibis.ipl.PortType;
import ibis.ipl.ReadMessage;
import ibis.ipl.ReceivePort;
import ibis.ipl.ReceivePortIdentifier;
import ibis.ipl.Registry;
import ibis.ipl.ResizeHandler;
import ibis.ipl.SendPort;
import ibis.ipl.SendPortIdentifier;
import ibis.ipl.StaticProperties;
import ibis.ipl.WriteMessage;
import ibis.ipl.ReceivePortConnectUpcall;
import ibis.ipl.SendPortConnectUpcall;

import ibis.util.Timer;
import ibis.util.Sequencer;

import java.io.IOException;
import java.io.Serializable;
import java.net.InetAddress;
import java.net.UnknownHostException;
import java.util.ArrayList;
import java.util.Properties;
import java.util.Random;
import java.util.Vector;

/* 
   One important invariant: there is only one thread per machine that spawns
   work.
   Another: there is only one lock: the global satin object.
   invariant: all jobs in the queue and outstandingJobsList have me as owner.
   invariant: all invocations records in use are in one of these lists:
     - onStack (being worked on)
     - OutstandingJobs (stolen)
     - q (spawned but not yet running)

   invariant: When running java code, the parentStamp, parentOwner and 
   parent contain the spawner of the work. (parent may be null when running
   the root node, or when the spawner is a remote machine).

   Satin.spawn gets the satin wrapper.
   This can be serialized, and run may be called.

   When a job is spawned, the RTS put a stamp on it.
   When a job is stolen the RTS puts an entry in a table.
   The runRemote method creates a return wrapper containing the return value.
   The runtime system sends the return value back, together with the
   original stamp. The victim can do a lookup to find the entry (containing
   the spawn counter and result pointer) that corresponds with the job.
*/

/**
 * This is the main class of the Ibis Satin implementation.
 * Its public methods are called by code generated by the Satin frontend,
 * not directly by the user.
 */
public final class Satin implements Config, ResizeHandler, 
				    ReceivePortConnectUpcall, SendPortConnectUpcall {

	Ibis ibis; //used in GlobalResultTable
	
	static Satin this_satin = null;

	IbisIdentifier ident; // used in messageHandler

	/* Options. */
	boolean closed = false; // used in TupleSpace
	boolean stats = true; // used in messageHandler
	private boolean detailedStats = false;
	private boolean ibisSerialization = true;
	private boolean upcallPolling = false;
	private int killTime = 0; //used in automatic ft tests
	private int deleteTime = 0;
	int branchingFactor = 0; //if > 0 and GLOBALLY_UNIQUE_STAMPS is set, it is used for generating globally unique stamps
	boolean getTable = true; //true if the node needs to download the contents of the global result table; protected by lock



	/* Am I the root (the one running main)? */
	boolean master = false;
	private String[] mainArgs;
	private String name;
	protected IbisIdentifier masterIdent;
	
	/* Am I the cluster coordinator? */
	boolean clusterCoordinator = false;
	IbisIdentifier clusterCoordinatorIdent;

        protected Sequencer sequencer; // used in MessageHandler
        protected int stealReplySeqNr;

	/* My scheduling algorithm. */
	protected final Algorithm algorithm;

	volatile int exitReplies = 0;
	
	int expected_seqno = Sequencer.START_SEQNO;

	// WARNING: dijkstra does not work in combination with aborts.
	DEQueue q = (ABORTS || FAULT_TOLERANCE) ? ((DEQueue) new DEQueueNormal(this)) : 
		((DEQueue) new DEQueueDijkstra(this));

	private PortType portType;
	private ReceivePort receivePort;
	private ReceivePort barrierReceivePort; /* Only for the master. */
	private SendPort barrierSendPort; /* Only for the clients. */
	SendPort tuplePort; /* used to bcast tuples*/
	//ft
	private long connectTimeout = 3000; /* Timeout for connecting to other nodes (in join()) who might be crashed */

	volatile boolean exiting = false; // used in messageHandler
	Random random = new Random(); // used in victimTable
	private MessageHandler messageHandler;

	private static SpawnCounter spawnCounterCache = null;

	/* Used to locate the invocation record corresponding to the
	   result of a remote job. */
	private IRVector outstandingJobs = new IRVector(this);
	private IRVector resultList = new IRVector(this);
	private ArrayList activeTupleKeyList = new ArrayList();
	private ArrayList activeTupleDataList = new ArrayList();
	private volatile boolean receivedResults = false;
	private int stampCounter = 0;

	private IRStack onStack = new IRStack(this);
	private IRVector exceptionList = new IRVector(this);

	/* abort messages are queued until the sync. */
	private StampVector abortList = new StampVector();
	
	/* used for fault tolerance */
	private StampVector abortAndStoreList;

	/* used to store reply messages */
	volatile boolean gotStealReply = false; // used in messageHandler
	volatile boolean gotBarrierReply = false; // used in messageHandler
	volatile boolean gotActiveTuples = false; // used in messageHandler
	protected final boolean upcalls;

	InvocationRecord stolenJob = null;
	IbisIdentifier stolenFrom = null;

	private int suggestedQueueSize = 1000;
        boolean tuplePortLocalConnection = false;

	/* Variables that contain statistics. */
	private long spawns = 0;
	private long syncs = 0;
	private long aborts = 0;
	private long jobsExecuted = 0;
	long abortedJobs = 0; // used in dequeue
	long abortMessages = 0;
	private long stealAttempts = 0;
	private long stealSuccess = 0;
	private long tupleMsgs = 0;
	private long tupleBytes = 0;	
	long killedOrphans = 0;
	long stolenJobs = 0; // used in messageHandler
	long stealRequests = 0; // used in messageHandler
	long interClusterMessages = 0;
	long intraClusterMessages = 0;
	long interClusterBytes = 0;
	long intraClusterBytes = 0;
	SatinStats totalStats; // used in messageHandler
	
	private int parentStamp = -1;
	private IbisIdentifier parentOwner = null;
	InvocationRecord parent = null;

	/* use these to avoid locking */
	private volatile boolean gotExceptions = false;
	private volatile boolean gotAborts = false;
	private volatile boolean gotCrashes = false;
	private volatile boolean gotAbortsAndStores = false;
	private volatile boolean gotDelete = false;
	
	/* used for fault tolerance
	   we must know who the current victim is, in case it crashes */
	IbisIdentifier currentVictim = null;
	boolean currentVictimCrashed = false;
	IbisIdentifier asyncCurrentVictim = null;
	boolean asyncCurrentVictimCrashed = false;
	
	/* historical name.. it's the global job table used in fault tolerance */
	GlobalResultTable globalResultTable = null;
	
	//used in ft, true if the master crashed and the whole work was restarted
	boolean restarted = false;
	
	
	//used for generating globally unique stamps, number of jobs spawned by root
	//no node can execute the root job twice, so this counter does not have to be set to 0 after root crash
	int rootNumSpawned = 0;
	
	
	//for debugging ft
	boolean del = false;
	private IbisIdentifier crashedIbis = null;
	Vector allIbises = new Vector();
	final static int NUM_CRASHES = 0;
	
	boolean connectionUpcallsDisabled = false;

	/* All victims, myself NOT included. The elements are Victims. */
	VictimTable victims;

	Timer totalTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer stealTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer handleStealTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer abortTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer idleTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer pollTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer tupleTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer invocationRecordWriteTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer invocationRecordReadTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer tupleOrderingWaitTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer tupleOrderingSeqTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer lookupTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer updateTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer handleUpdateTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer handleLookupTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer crashTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer redoTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	Timer addReplicaTimer = Timer.newTimer("ibis.util.nativeCode.Rdtsc");
	long prevPoll = 0;
//	float MHz = Timer.getMHz();

	java.io.PrintStream out = System.out;
	
	/* Used for fault tolerance
	   Ibises that crashed recently and whose crashes still need to be handled*/
	Vector crashedIbises = new Vector(); 
	/* Used for fault tolerance
	   All ibises that once took part in the computation, but then crashed 
	   Assumption: ibis identifiers are uniqe in time; the same ibis cannot crash
	   and join the computation again*/
	Vector deadIbises = new Vector(); 


	/**
	 * Creates a Satin instance and also an Ibis instance to run Satin on.
	 * This constructor gets called by the rewritten main() from the
	 * application, and the argument array from main is passed to
	 * this constructor.
	 * Which ibis is chosen depends, a.o., on these arguments.
	 * All flags destined for Satin start with "satin_".
	 * The flags that are recognized are:
	 * <table>
	 * <tr>
	 * <td>-satin-closed</td>
	 * <td>all members join the run during startup.</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-panda</td>
	 * <td>use the Panda version of Ibis.</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-mpi</td>
	 * <td>use the MPI version of Ibis.</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-net</td>
	 * <td>use NetIBis</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-nio</td>
	 * <td>use NIO Ibis</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-tcp</td>
	 * <td>use TCP Ibis</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-stats</td>
	 * <td>display statistics at the end of the run</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-no-stats</td>
	 * <td>don't display statistics at the end of the run</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-detailed-stats</td>
	 * <td>display detailed statistics for every member at the end of the run</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-ibis</td>
	 * <td>use Ibis serialization</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-sun</td>
	 * <td>use Sun serialization</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-no-upcalls</td>
	 * <td>use explicit receive for receiving messages</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-upcalls</td>
	 * <td>use upcalls for receiving messages</td>
	 * </tr>
	 * <tr>
	 * <td>-satin-upcall-polling</td>
	 * <td>use upcalls for receiving messages, explicit poll is required
	 * to get the upcall. </td>
	 * </tr>
	 * <tr>
	 * <td nowrap>-satin-queue-size <em>num</em></td>
	 * <td>suggested job queue size.
	 * </tr>
	 * <tr>
	 * <td>-satin-alg <em>alg</em></td>
	 * <td>there are currently three job-stealing algorithms:
	 * "RS" (random work stealing), "CRS" (cluster aware random work
	 * stealing), and "MW" (master worker).</td>
	 * </tr>
	 * <tr>
	 * <td> -satin-kill <em>sec</em></td>
	 * <td>crash after <em>sec</em> seconds; used for tests</td>
	 * </tr>
	 * <tr>
	 * <td> -satin-delete <em>sec</em></td>
	 * <td>leave after <em>sec</em> seconds; used for tests</td>
	 * </tr>
	 * <tr>
	 * <td> -satin-branching-factor <em>num</em></td>
	 * <td>the maximal branching factor of the application
	 * (the maximal number of subjobs spawned by any job);
	 * used for generating stamps</td>
	 * </tr>
	 * <tr>
	 * </table>
	 */
	public Satin(String[] args) {

		if(this_satin != null) {
			throw new IbisError("multiple satin instances are currently not supported");
		}
		this_satin = this;

		if(stealTimer == null) {
			System.err.println("Native timers not found, using (less accurate) java timers.");
		}

		if(totalTimer == null) totalTimer = new Timer();
		if(stealTimer == null) stealTimer = new Timer();
		if(handleStealTimer == null) handleStealTimer = new Timer();
		if(abortTimer == null) abortTimer = new Timer();
		if(idleTimer == null) idleTimer = new Timer();
		if(pollTimer == null) pollTimer = new Timer();
		if(tupleTimer == null) tupleTimer = new Timer();
		if(invocationRecordWriteTimer == null) invocationRecordWriteTimer = new Timer();
		if(invocationRecordReadTimer == null) invocationRecordReadTimer = new Timer();
		if(tupleOrderingWaitTimer == null) tupleOrderingWaitTimer = new Timer();
		if(tupleOrderingSeqTimer == null) tupleOrderingSeqTimer = new Timer();
		if(lookupTimer == null) lookupTimer = new Timer();
		if(updateTimer == null) updateTimer = new Timer();
		if(handleUpdateTimer == null) handleUpdateTimer = new Timer();
		if(handleLookupTimer == null) handleLookupTimer = new Timer();
		if(crashTimer == null) crashTimer = new Timer();
		if(redoTimer == null) redoTimer = new Timer();
		if(addReplicaTimer == null) addReplicaTimer = new Timer();

		Properties p = System.getProperties();
		String hostName = null;
		String alg = null;
		int poolSize = 0; /* Only used with closed world. */
		

		try {
			InetAddress address = InetAddress.getLocalHost();
			hostName = address.getHostName();

		} catch (UnknownHostException e) {
			System.err.println("SATIN:init: Cannot get ip of local host: " + e);
			System.exit(1);
		}

		StaticProperties reqprops = new StaticProperties();
		reqprops.add("serialization", "object");

		boolean doUpcalls = true;

		/* Parse commandline parameters. Remove everything that starts
		   with satin. */
		Vector tempArgs = new Vector();
		for(int i=0; i<args.length; i++) {
			if(args[i].equals("-satin-closed")) {/* Closed world assumption. */
				closed = true;
			} else if(args[i].equals("-satin-panda")) {
				reqprops.add("name", "panda");
			} else if(args[i].equals("-satin-mpi")) {
				reqprops.add("name", "mpi");
			} else if(args[i].startsWith("-satin-net")) {
				reqprops.add("name", args[i].substring(7));
			} else if(args[i].equals("-satin-nio")) {
				reqprops.add("name", "nio");
			} else if(args[i].equals("-satin-tcp")) {
				reqprops.add("name", "tcp");
			} else if(args[i].equals("-satin-stats")) {
				stats = true;
			} else if(args[i].equals("-satin-no-stats")) {
				stats = false;
			} else if(args[i].equals("-satin-detailed-stats")) {
				stats = true;
				detailedStats = true;
			} else if(args[i].equals("-satin-ibis")) {
				ibisSerialization = true;
			} else if(args[i].equals("-satin-sun")) {
				ibisSerialization = false;
			} else if(args[i].equals("-satin-no-upcalls")) {
				doUpcalls = false;
			} else if(args[i].equals("-satin-upcalls")) {
				doUpcalls = true;
			} else if(args[i].equals("-satin-upcall-polling")) {
				upcallPolling = true;
			} else if(args[i].equals("-satin-queue-size")) {
				i++;
				try {
					suggestedQueueSize = Integer.parseInt(args[i]);
				} catch (Exception e) {
					System.err.println("Option -satin-queue-size needs integer parameter.");
					System.exit(1);
				}
			} else if(args[i].equals("-satin-alg")) {
				i++;
				alg = args[i];
			} else if(args[i].equals("-satin-delete")) {
				i++;
				try {
				    deleteTime = Integer.parseInt(args[i]);
				} catch (NumberFormatException e) {
				    System.err.println("SATIN: invalid delete time");
				}
			} else if(args[i].equals("-satin-kill")) {
				i++;
				try {
				    killTime = Integer.parseInt(args[i]);
				} catch (NumberFormatException e) {
				    System.err.println("SATIN: invalid kill time");
				}
			} else if(args[i].equals("-satin-branching-factor")) {
				i++;
				try {
				    branchingFactor = Integer.parseInt(args[i]);
				} catch (NumberFormatException e) {
				    System.err.println("SATIN: invalid branching factor");
				}
			} else {
				tempArgs.add(args[i]);
			}
		}

		// Combine old-style arguments with new style properties.

		StaticProperties userprops = StaticProperties.userProperties();
		String str = userprops.getProperty("worldmodel");
		if (str != null) {
		    if (closed && ! str.equals("closed")) {
			System.err.println("Inconsistent options: -satin-closed and -Dibis.worldmodel=" + str);
			System.exit(1);
		    }
		    if (str.equals("closed")) {
			closed = true;
		    }
		}
		str = userprops.getProperty("name");
		if (str != null) {
		    String s2 = reqprops.getProperty("name");
		    if (s2 != null && ! s2.equals(str)) {
			System.err.println("Inconsistent options: -satin-" + s2 + " and -Dibis.name=" + str);
			System.exit(1);
		    }
		}
		str = userprops.getProperty("serialization");
		if (str != null) {
		    if (ibisSerialization && ! str.equals("ibis")) {
			System.err.println("Inconsistent options: -satin-ibis and -Dibis.serialization=" + str);
			System.exit(1);
		    }
		    if (str.equals("ibis")) {
			ibisSerialization = true;
		    }
		}

		if (closed) {
		    reqprops.add("worldmodel", "closed");
		}
		else {
		    reqprops.add("worldmodel", "open");
		}

		if (doUpcalls) {
		    if (upcallPolling) {
			reqprops.add("communication", "OneToOne, OneToMany, ManyToOne, Reliable, PollUpcalls, ExplicitReceipt, ConnectionUpcalls");
		    }
		    else {
			reqprops.add("communication", "OneToOne, OneToMany, ManyToOne, Reliable, AutoUpcalls, ExplicitReceipt, ConnectionUpcalls");
		    }
		}
		else {
		    reqprops.add("communication", "OneToOne, OneToMany, ManyToOne, Reliable, ExplicitReceipt, ConnectionUpcalls");
		}

		upcalls = doUpcalls; // upcalls is final for performance reasons :-)

		mainArgs = new String[tempArgs.size()];
		for(int i=0; i<tempArgs.size(); i++) {
			mainArgs[i] = (String) tempArgs.get(i);
		}

		if(closed) {
			String pool = p.getProperty("ibis.pool.total_hosts");
			if(pool == null) {
				out.println("property 'ibis.pool.total_hosts' not set," +
					    " and running with closed world.");
				System.exit(1);
			}

			poolSize = Integer.parseInt(pool);
		}

		if(COMM_DEBUG) {
			out.println("SATIN '" + hostName + "': init ibis" );
		}

		try {
			ibis = Ibis.createIbis(reqprops, this);

		} catch (IbisException e) {
			System.err.println("SATIN '" + hostName + 
					   "': Could not start ibis: " + e.getMessage());
//			e.printStackTrace();
			System.exit(1);
		}

		ident = ibis.identifier();

		parentOwner = ident;

		victims = new VictimTable(this); //victimTable accesses ident..

		if(COMM_DEBUG) {
			out.println("SATIN '" + hostName + "': init ibis DONE, " +
				    "my cluster is '" + ident.cluster() + "'");
		}

		try {
			StaticProperties s = new StaticProperties(reqprops);
			Registry r = ibis.registry();

			masterIdent = (IbisIdentifier) r.elect("satin master", ident);

			if(masterIdent.equals(ident)) {
				/* I an the master. */
				if(COMM_DEBUG) {
					out.println("SATIN '" + hostName +
						    "': init ibis: I am the master");
				}
				System.out.println("master is " + hostName);
				master = true;
			} else {
				if(COMM_DEBUG) {
					out.println("SATIN '" + hostName +
						    "': init ibis I am slave" );
				}
			}
			
			if (FAULT_TOLERANCE) {
			    clusterCoordinatorIdent = (IbisIdentifier) r.elect("satin " + ident.cluster() + " cluster coordinator", ident);
			    if (clusterCoordinatorIdent.equals(ident)) {
				    /* I am the cluster coordinator */
				    clusterCoordinator = true;
				    System.out.println("cluster coordinator for cluster " + ident.cluster() + " is " + hostName);
			    }
			}

			if(ibisSerialization) {
				s.add("Serialization", "ibis");
				if(master) {
					System.err.println("SATIN: using Ibis serialization");
				}
			}

			portType = ibis.createPortType("satin porttype", s);

			messageHandler = new MessageHandler(this);

			if(upcalls) {
				receivePort = portType.createReceivePort("satin port on " + 
									 ident.name(), messageHandler, this);
			} else {
				if(master) {
					System.err.println("SATIN: using blocking receive");
				}
				receivePort = portType.createReceivePort("satin port on " + 
									 ident.name(), this);
			}

			if(master) {
				barrierReceivePort =
					portType.createReceivePort("satin barrier receive port on " + ident.name());
				barrierReceivePort.enableConnections();
			} else {
				barrierSendPort =
					portType.createSendPort("satin barrier send port on " + 
								ident.name());
				ReceivePortIdentifier barrierIdent =
					lookup("satin barrier receive port on " + masterIdent.name());
				connect(barrierSendPort, barrierIdent);
			}

			// Create a multicast port to bcast tuples.
			// Connections are established in the join upcall.
			if(SUPPORT_TUPLE_MULTICAST) {
				tuplePort = 
					portType.createSendPort("satin tuple port on " +
								ident.name());
			}
		} catch (Exception e) {
			System.err.println("SATIN '" + hostName +
					   "': Could not start ibis: " + e);
			e.printStackTrace();
			System.exit(1);
		}

		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': init ibis DONE2");
		}

		if(stats && master) {
			totalStats = new SatinStats();
		}

		if(master) {
			if(closed) {
				System.err.println("SATIN '" + hostName +
						   "': running with closed world, "
						   + poolSize + " host(s)");
			} else {
				System.err.println("SATIN '" + hostName +
						   "': running with open world");
			}
		}

		if(alg == null) {
			if(master) {
				System.err.println("SATIN '" + hostName +
						   "': satin_algorithm property not specified, using RS");
			}
			alg = "RS";
		}

		if(alg.equals("RS")) {
			algorithm = new RandomWorkStealing(this);
		} else if(alg.equals("CRS")) {
			algorithm = new ClusterAwareRandomWorkStealing(this);
		} else if(alg.equals("MW")) {
			algorithm = new MasterWorker(this);
		} else {
			System.err.println("SATIN '" + hostName + "': satin_algorithm '"
					   + alg + "' unknown");
			algorithm = null;
			System.exit(1);
		}
		
		if(COMM_DEBUG) {
		    out.println("SATIN '" + hostName + "': algorithm created");
		}

		if(upcalls) receivePort.enableUpcalls();
		receivePort.enableConnections();
		
		if(FAULT_TOLERANCE) {
		    globalResultTable = new GlobalResultTable(this);
		}
		if(FAULT_TOLERANCE) {
		    abortAndStoreList = new StampVector();
		}
		
		if(FAULT_TOLERANCE && master) {
		    getTable = false;
		}
				
		ibis.openWorld();

		if(COMM_DEBUG) {
			out.println("SATIN '" + hostName + "': pre barrier" );
		}

		if(closed) {
			synchronized(this) {
				while(victims.size() != poolSize - 1) {
					try {
						wait();
					} catch (InterruptedException e) {
						System.err.println("eek: " + e);
						// Ignore.
					}
				}
				if(COMM_DEBUG) {
					out.println("SATIN '" + hostName +
						    "': barrier, everybody has joined" );
				}

				ibis.closeWorld();
			}

			if (SatinTupleSpace.use_seq) {
				enableActiveTupleOrdening();
			}

			barrier();
			
			if (killTime > 0) {
			    (new KillerThread(killTime)).start();
			}
			if (deleteTime > 0) {
			    (new DeleteThread(this, deleteTime)).start();
			}
		}

		if(COMM_DEBUG) {
			out.println("SATIN '" + hostName + "': post barrier" );
		}

		if (ADD_REPLICA_TIMING) {
		    if (!master) {
			addReplicaTimer.start();
		    }
		}
		totalTimer.start();
	}

        void enableActiveTupleOrdening() {
	        if(tuplePortLocalConnection) return;

	        connect(tuplePort, receivePort.identifier());
		try {
		        sequencer = Sequencer.getSequencer(ibis);
		} catch(IOException e) {
		        System.err.println("SATIN '" + ident.name() + 
					   "': Got Exception while creating sequencer: " + e);
			System.exit(1);
		}
		tuplePortLocalConnection = true;
	}

	/**
	 * Returns true if this is the instance that is running main().
	 * @return <code>true</code> if this is the instance running main().
	 */
	public boolean isMaster() {
		return master;
	}

	/**
	 * Returns the argument list that is left when Satin has stripped
	 * its arguments from it.
	 */
	public String[] getMainArgs() {
		return mainArgs;
	}

	/**
	 * Returns the invocation record that is the "parent" of the current
	 * work. May return <code>null</code> when not processing spawned work
	 * yet.
	 * @return the "current" invocation record.
	 */
	public InvocationRecord getParent() {
		return parent;
	}


	boolean inDifferentCluster(IbisIdentifier other) {
		if (ASSERTS) {
			if (ident.cluster() == null || other.cluster() == null) {
				System.err.println("WARNING: Found NULL cluster!");

				/* this isn't severe enough to exit, so return something */
				return true;
			}
		}

		return !ident.cluster().equals(other.cluster());
	}

	private void printStats() {
		int size;

		synchronized(this) {
			// size = victims.size();
			// No, this is one too few. (Ceriel)
			size = victims.size() + 1;
		}

		// add my own stats
		SatinStats me = createStats();
		totalStats.add(me);

		java.text.NumberFormat nf = java.text.NumberFormat.getInstance();
//		pf.setMaximumIntegerDigits(3);
//		pf.setMinimumIntegerDigits(3);

		// for percentages
		java.text.NumberFormat pf = java.text.NumberFormat.getInstance();
		pf.setMaximumFractionDigits(3);
		pf.setMinimumFractionDigits(3);
		pf.setGroupingUsed(false);


		out.println("-------------------------------SATIN STATISTICS--------------------------------");
		if(SPAWN_STATS) {
			out.println("SATIN: SPAWN:       " + nf.format(totalStats.spawns) +
				    " spawns, " + nf.format(totalStats.jobsExecuted) + 
				    " executed, " + nf.format(totalStats.syncs) + " syncs");
			if(ABORTS) {
				out.println("SATIN: ABORT:       " +
					    nf.format(totalStats.aborts) +
					    " aborts, " + nf.format(totalStats.abortMessages) +
					    " abort msgs, " + nf.format(totalStats.abortedJobs) + " aborted jobs");
			}
		}

		if(TUPLE_STATS) {
			out.println("SATIN: TUPLE_SPACE: " + 
				    nf.format(totalStats.tupleMsgs) +
				    " bcasts, " + nf.format(totalStats.tupleBytes) + " bytes");
		}

		if(POLL_FREQ != 0 && POLL_TIMING) {
			out.println("SATIN: POLL:        poll count = " + nf.format(totalStats.pollCount));
		}

		if(STEAL_STATS) {
			out.println("SATIN: STEAL:       " + 
				    nf.format(totalStats.stealAttempts) +
				    " attempts, " + nf.format(totalStats.stealSuccess) + " successes (" +
				    pf.format(((double) totalStats.stealSuccess / totalStats.stealAttempts) * 100.0) +
				    " %)");

			out.println("SATIN: MESSAGES:    intra " + 
				    nf.format(totalStats.intraClusterMessages) +
				    " msgs, " + nf.format(totalStats.intraClusterBytes) +
				    " bytes; inter " + nf.format(totalStats.interClusterMessages) +
				    " msgs, " + nf.format(totalStats.interClusterBytes) + " bytes");
		}
		
		if(GRT_STATS) {
			out.println("SATIN: GLOBAL_RESULT_TABLE: updates " + nf.format(totalStats.tableUpdates) +
				    ",lookups " + nf.format(totalStats.tableLookups) + 
				    ",successful " + nf.format(totalStats.tableSuccessfulLookups) +
				    ",remote " + nf.format(totalStats.tableRemoteLookups));
		}
		
		if(FT_ABORT_STATS) {
			out.println("SATIN: ORPHAN JOBS: killed orphans " + nf.format(totalStats.killedOrphans));
		}
				    

		out.println("-------------------------------SATIN TOTAL TIMES-------------------------------");
		if(STEAL_TIMING) {
			out.println("SATIN: STEAL_TIME:             total " +
				    Timer.format(totalStats.stealTime) + " time/req    " +
				    Timer.format(totalStats.stealTime / totalStats.stealAttempts));
				    out.println("SATIN: HANDLE_STEAL_TIME:      total " + 
				    Timer.format(totalStats.handleStealTime) +
				    " time/handle " + 
				    Timer.format((totalStats.handleStealTime)/totalStats.stealAttempts));

				    out.println("SATIN: SERIALIZATION_TIME:     total " + 
				    Timer.format(totalStats.invocationRecordWriteTime) +
				    " time/write  " + 
				    Timer.format(totalStats.invocationRecordWriteTime/totalStats.stealSuccess));
				    out.println("SATIN: DESERIALIZATION_TIME:   total " + 
				    Timer.format(totalStats.invocationRecordReadTime) +
				    " time/read   " + 
				    Timer.format(totalStats.invocationRecordReadTime/totalStats.stealSuccess));
			}

		if(ABORT_TIMING) {
			out.println("SATIN: ABORT_TIME:             total " + 
				    Timer.format(totalStats.abortTime) +
				  " time/abort  " + Timer.format(totalStats.abortTime / totalStats.aborts));
		}

		if(TUPLE_TIMING) {
			out.println("SATIN: TUPLE_SPACE_BCAST_TIME: total " +
				    Timer.format(totalStats.tupleTime) + " time/bcast  " +
				    Timer.format(totalStats.tupleTime/totalStats.tupleMsgs));
			out.println("SATIN: TUPLE_SPACE_WAIT_TIME:  total " +
				    Timer.format(totalStats.tupleWaitTime) + " time/bcast  " +
				    Timer.format(totalStats.tupleWaitTime/totalStats.tupleWaitCount));
			out.println("SATIN: TUPLE_SPACE_ORDER_TIME: total " +
				    Timer.format(totalStats.tupleSeqTime) + " time/bcast  " +
				    Timer.format(totalStats.tupleSeqTime/totalStats.tupleSeqCount));
		}

		if(POLL_FREQ != 0 && POLL_TIMING) {
			out.println("SATIN: POLL_TIME:            total " +
				    Timer.format(totalStats.pollTime) + " time/poll " +
				    Timer.format(totalStats.pollTime/totalStats.pollCount));
		}
			
		if(GRT_TIMING) {
			out.println("SATIN: GRT_UPDATE_TIME:		  total " +
				    Timer.format(totalStats.tableUpdateTime) + " time/update " +
				    Timer.format(totalStats.tableUpdateTime/totalStats.tableUpdates));
			out.println("SATIN: GRT_LOOKUP_TIME:		  total " +
				    Timer.format(totalStats.tableLookupTime) + " time/lookup " +
				    Timer.format(totalStats.tableLookupTime/totalStats.tableLookups));
			out.println("SATIN: GRT_HANDLE_UPDATE_TIME:	  total " +
				    Timer.format(totalStats.tableHandleUpdateTime) + " time/handle " +
				    Timer.format(totalStats.tableHandleUpdateTime/totalStats.tableUpdates*(size-1)));
			out.println("SATIN: GRT_HANDLE_LOOKUP_TIME: 	  total " +
				    Timer.format(totalStats.tableHandleLookupTime) + " time/handle " +
				    Timer.format(totalStats.tableHandleLookupTime/totalStats.tableRemoteLookups));
		}
		
		if(CRASH_TIMING) {
			out.println("SATIN: CRASH_HANDLING_TIME: 	  " + Timer.format(totalStats.crashHandlingTime));
		}
		
		if(ADD_REPLICA_TIMING) {
			out.println("SATIN: ADD_REPLICA_TIME:	 	  " + Timer.format(totalStats.addReplicaTime));
		}
			
			

			out.println("-------------------------------SATIN RUN TIME BREAKDOWN------------------------");
			out.println("SATIN: TOTAL_RUN_TIME:                           " +
				    Timer.format(totalTimer.totalTimeVal()));

		double lbTime = (totalStats.stealTime - totalStats.invocationRecordReadTime -
				 totalStats.invocationRecordWriteTime) / size;
		if(lbTime < 0.0) lbTime = 0.0;
		double lbPerc = lbTime/totalTimer.totalTimeVal() * 100.0;
		double serTime = (totalStats.invocationRecordWriteTime +
				  totalStats.invocationRecordReadTime) / size;
		double serPerc = serTime/totalTimer.totalTimeVal() * 100.0;
		double abortTime = totalStats.abortTime / size;
		double abortPerc = abortTime/totalTimer.totalTimeVal() * 100.0;
		double tupleTime = totalStats.tupleTime / size;
		double tuplePerc = tupleTime/totalTimer.totalTimeVal() * 100.0;
		double tupleWaitTime = totalStats.tupleWaitTime / size;
		double tupleWaitPerc = tupleWaitTime/totalTimer.totalTimeVal() * 100.0;
		double tupleSeqTime = totalStats.tupleSeqTime / size;
		double tupleSeqPerc = tupleSeqTime/totalTimer.totalTimeVal() * 100.0;
		double pollTime = totalStats.pollTime / size;
		double pollPerc = pollTime/totalTimer.totalTimeVal() * 100.0;
		
		double tableUpdateTime = totalStats.tableUpdateTime / size;
		double tableUpdatePerc = tableUpdateTime/totalTimer.totalTimeVal() * 100.0;
		double tableLookupTime = totalStats.tableLookupTime / size;
		double tableLookupPerc = tableLookupTime/totalTimer.totalTimeVal() * 100.0;
		double tableHandleUpdateTime = totalStats.tableHandleUpdateTime / size;
		double tableHandleUpdatePerc = tableHandleUpdateTime/totalTimer.totalTimeVal() * 100.0;
		double tableHandleLookupTime = totalStats.tableHandleLookupTime / size;
		double tableHandleLookupPerc = tableHandleLookupTime/totalTimer.totalTimeVal() * 100.0;
		double crashHandlingTime = totalStats.crashHandlingTime / size;
		double crashHandlingPerc = crashHandlingTime/totalTimer.totalTimeVal() * 100.0;
		double addReplicaTime = totalStats.addReplicaTime / size;
		double addReplicaPerc = addReplicaTime/totalTimer.totalTimeVal() * 100.0;
			
			
		double totalOverhead = lbTime + serTime + abortTime + 
		    tupleTime + tupleWaitTime + pollTime;
		double totalPerc = totalOverhead/totalTimer.totalTimeVal() * 100.0;
		double appTime = totalTimer.totalTimeVal() - totalOverhead;
		if(appTime < 0.0) appTime = 0.0;
		double appPerc = appTime/totalTimer.totalTimeVal() * 100.0;
		

		if(STEAL_TIMING) {
			out.println("SATIN: LOAD_BALANCING_TIME:     avg. per machine " +
				    Timer.format(lbTime) + " (" + 
				    (lbPerc < 10 ? " ": "" ) +
				    pf.format(lbPerc) + " %)");
			out.println("SATIN: (DE)SERIALIZATION_TIME:  avg. per machine " + 
				    Timer.format(serTime) + " (" +
				    (serPerc < 10 ? " ": "" ) +
				    pf.format(serPerc) + " %)");
		}
			

		if(ABORT_TIMING) {
			out.println("SATIN: ABORT_TIME:              avg. per machine " + 
				    Timer.format(abortTime) + " (" +
				    (abortPerc < 10 ? " ": "" ) +
				    pf.format(abortPerc) + " %)");
		}
		
				
		if(TUPLE_TIMING) {
			out.println("SATIN: TUPLE_SPACE_BCAST_TIME:  avg. per machine " +
				    Timer.format(tupleTime) + " (" +
				    (tuplePerc < 10 ? " ": "" ) +
				    pf.format(tuplePerc) + " %)");
			out.println("SATIN: TUPLE_SPACE_WAIT_TIME:   avg. per machine " +
				    Timer.format(tupleWaitTime) + " (" +
				    (tupleWaitPerc < 10 ? " ": "" ) +
				    pf.format(tupleWaitPerc) + " %)");
			out.println("SATIN: TUPLE_SPACE_ORDER_TIME:  avg. per machine " +
				    Timer.format(tupleSeqTime) + " (" +
				    (tupleSeqPerc < 10 ? " ": "" ) +
				    pf.format(tupleSeqPerc) + " %)");
		}
			
		if(POLL_FREQ != 0 && POLL_TIMING) {
			out.println("SATIN: POLL_TIME:               avg. per machine " +
				    Timer.format(pollTime) + " (" +
				    (pollPerc < 10 ? " ": "" ) +
				    pf.format(pollPerc) + " %)");
		}
			

		if(GRT_TIMING) {
			out.println("SATIN: GRT_UPDATE_TIME:              avg. per machine " +
				    Timer.format(tableUpdateTime) + " (" +
				    pf.format(tableUpdatePerc) + " %)");
			out.println("SATIN: GRT_LOOKUP_TIME:              avg. per machine " +
				    Timer.format(tableLookupTime) + " (" +
				    pf.format(tableLookupPerc) + " %)");
			out.println("SATIN: GRT_HANDLE_UPDATE_TIME:       avg. per machine " +
				    Timer.format(tableHandleUpdateTime) + " (" +
				    pf.format(tableHandleUpdatePerc) + " %)");
			out.println("SATIN: GRT_HANDLE_LOOKUP_TIME:       avg. per machine " +
				    Timer.format(tableHandleLookupTime) + " (" +
				    pf.format(tableHandleLookupPerc) + " %)");
		}
				    				    

		if(CRASH_TIMING) {
			out.println("SATIN: CRASH_HANDLING_TIME: 	  avg. per machine " +
				    Timer.format(crashHandlingTime) + " (" +
				    pf.format(crashHandlingPerc) + " %)");
		}

		if(ADD_REPLICA_TIMING) {
			out.println("SATIN: ADD_REPLICA_TIME:	 	  avg. per machine " +
				    Timer.format(addReplicaTime) + " (" +
				    pf.format(addReplicaPerc) + " %)");
		}


		out.println("\nSATIN: TOTAL_PARALLEL_OVERHEAD: avg. per machine " +
			    Timer.format(totalOverhead) + " (" +
			    (totalPerc < 10 ? " ": "" ) +
			    pf.format(totalPerc) + " %)");

		out.println("SATIN: USEFUL_APP_TIME:         avg. per machine " +
			    Timer.format(appTime) + " (" +
			    (appPerc < 10 ? " ": "" ) +
			    pf.format(appPerc) + " %)");

	}

	private void printDetailedStats() {
		java.text.NumberFormat nf = java.text.NumberFormat.getInstance();

		if(SPAWN_STATS) {
			out.println("SATIN '" + ident.name() + 
				    "': SPAWN_STATS: spawns = " + spawns +
				    " executed = " + jobsExecuted + 
				    " syncs = " + syncs);
			if(ABORTS) {
				out.println("SATIN '" + ident.name() + 
					    "': ABORT_STATS 1: aborts = " + aborts +
					    " abort msgs = " + abortMessages +
					    " aborted jobs = " + abortedJobs);
			}
		}
		if(TUPLE_STATS) {
			out.println("SATIN '" + ident.name() + 
				    "': TUPLE_STATS 1: tuple bcast msgs: " + tupleMsgs +
				    ", bytes = " + nf.format(tupleBytes));
		}
		if(STEAL_STATS) {
			out.println("SATIN '" + ident.name() + 
				    "': INTRA_STATS: messages = " + intraClusterMessages +
				    ", bytes = " + nf.format(intraClusterBytes));

			out.println("SATIN '" + ident.name() + 
				    "': INTER_STATS: messages = " + interClusterMessages +
				    ", bytes = " + nf.format(interClusterBytes));

			out.println("SATIN '" + ident.name() + 
				    "': STEAL_STATS 1: attempts = " + stealAttempts +
				    " success = " + stealSuccess + " (" +
				    (((double) stealSuccess / stealAttempts) * 100.0) +
				    " %)");

			out.println("SATIN '" + ident.name() + 
				    "': STEAL_STATS 2: requests = " + stealRequests +
				    " jobs stolen = " + stolenJobs);

			if(STEAL_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': STEAL_STATS 3: attempts = " +
					    stealTimer.nrTimes() + " total time = " +
					    stealTimer.totalTime() + " avg time = " +
					    stealTimer.averageTime());

				out.println("SATIN '" + ident.name() + 
					    "': STEAL_STATS 4: handleSteals = " +
					    handleStealTimer.nrTimes() + 
					    " total time = " + handleStealTimer.totalTime() +
					    " avg time = " + handleStealTimer.averageTime());
				out.println("SATIN '" + ident.name() + 
					    "': STEAL_STATS 5: invocationRecordWrites = " +
					    invocationRecordWriteTimer.nrTimes() + 
					    " total time = " + invocationRecordWriteTimer.totalTime() +
					    " avg time = " + invocationRecordWriteTimer.averageTime());
				out.println("SATIN '" + ident.name() + 
					    "': STEAL_STATS 6: invocationRecordReads = " +
					    invocationRecordReadTimer.nrTimes() + 
					    " total time = " + invocationRecordReadTimer.totalTime() +
					    " avg time = " + invocationRecordReadTimer.averageTime());
			}

			if(ABORTS && ABORT_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': ABORT_STATS 2: aborts = " +
					    abortTimer.nrTimes() + 
					    " total time = " + abortTimer.totalTime() +
					    " avg time = " + abortTimer.averageTime());
			}

			if(IDLE_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': IDLE_STATS: idle count = " +
					    idleTimer.nrTimes() + " total time = " +
					    idleTimer.totalTime() + " avg time = " +
					    idleTimer.averageTime());
			}

			if(POLL_FREQ != 0 && POLL_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': POLL_STATS: poll count = " +
					    pollTimer.nrTimes() + " total time = " +
					    pollTimer.totalTime() + " avg time = " +
					    pollTimer.averageTime());
			}

			if(STEAL_TIMING && IDLE_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': COMM_STATS: software comm time = " +
					    pollTimer.format(stealTimer.totalTimeVal() +
							     handleStealTimer.totalTimeVal() -
							     idleTimer.totalTimeVal()));
			}

			if(TUPLE_TIMING) {
				out.println("SATIN '" + ident.name() + 
					    "': TUPLE_STATS 2: bcasts = " +
					    tupleTimer.nrTimes() + " total time = " +
					    tupleTimer.totalTime() + " avg time = " +
					    tupleTimer.averageTime());

				out.println("SATIN '" + ident.name() + 
					    "': TUPLE_STATS 3: waits = " +
					    tupleOrderingWaitTimer.nrTimes() + " total time = " +
					    tupleOrderingWaitTimer.totalTime() + " avg time = " +
					    tupleOrderingWaitTimer.averageTime());

				out.println("SATIN '" + ident.name() + 
					    "': TUPLE_STATS 4: sequencer accesses = " +
					    tupleOrderingSeqTimer.nrTimes() + " total time = " +
					    tupleOrderingSeqTimer.totalTime() + " avg time = " +
					    tupleOrderingSeqTimer.averageTime());
			}
			algorithm.printStats(out);
		}
		
		if (FAULT_TOLERANCE) {
		    if (GRT_STATS) {
			out.println("SATIN '" + ident.name() + "': " + globalResultTable.numUpdates + " updates of the table.");
			out.println("SATIN '" + ident.name() + "': " + globalResultTable.numLookupsSucceded + " lookups succeded, of which:");
			out.println("SATIN '" + ident.name() + "': " + globalResultTable.numRemoteLookups + " remote lookups.");
			out.println("SATIN '" + ident.name() + "': " + globalResultTable.maxNumEntries + " entries maximally.");
		    }
		    if (GRT_TIMING) {
			out.println("SATIN '" + ident.name() + "': " + lookupTimer.totalTime() + " spent in lookups");
			out.println("SATIN '" + ident.name() + "': " + lookupTimer.averageTime() + " per lookup");
			out.println("SATIN '" + ident.name() + "': " + updateTimer.totalTime() + " spent in updates");
			out.println("SATIN '" + ident.name() + "': " + updateTimer.averageTime() + " per update");
			out.println("SATIN '" + ident.name() + "': " + handleUpdateTimer.totalTime() + " spent in handling updates");
			out.println("SATIN '" + ident.name() + "': " + handleUpdateTimer.averageTime() + " per update handle");
			out.println("SATIN '" + ident.name() + "': " + handleLookupTimer.totalTime() + " spent in handling lookups");
			out.println("SATIN '" + ident.name() + "': " + handleLookupTimer.averageTime() + " per lookup handle");
			
		    }
		    if (CRASH_TIMING) {
			out.println("SATIN '" + ident.name() + "': " + crashTimer.totalTime() + " spent in handling crashes");
		    }
		    if (TABLE_CHECK_TIMING) {
			out.println("SATIN '" + ident.name() + "': " + redoTimer.totalTime() + " spent in redoing");
		    
		    }
		    
		    if (FT_ABORT_STATS) {
			out.println("SATIN '" + ident.name() + "': " + killedOrphans + " orphans killed");
		    }
		}	    
	}
	
	/**
	 * Called at the end of the rewritten "main", to do a synchronized
	 * exit.
	 */
	public void exit() {
		/* send exit messages to all others */
		int size;

		totalTimer.stop();

		if(!closed) {
			ibis.closeWorld();
		}

		if(stats && detailedStats) printDetailedStats();

		    
		connectionUpcallsDisabled = true;
		
		synchronized(this) {
		    size = victims.size();
		    //System.err.println("victims size: " + size);
		}
		    

		if(master) {
			exiting = true;
			algorithm.exit(); // give the algorithm time to clean up

			for (int i=0; i<size; i++) {
				try {
					WriteMessage writeMessage;
					synchronized(this) {
						if(COMM_DEBUG) {
							out.println("SATIN '" + ident.name() + 
								    "': sending exit message to " +
								    victims.getIdent(i));
						}
						
						//System.err.println("victims size: " + victims.size() + ",i: " + i);
						writeMessage = victims.getPort(i).newMessage();
					}

					writeMessage.writeByte(Protocol.EXIT);
					writeMessage.finish();
				} catch (IOException e) {
					synchronized(this) {
						System.err.println("SATIN: Could not send exit message to " + victims.getIdent(i));
					}
				}
			}
			
			while(exitReplies != size) {
				satinPoll();
			}
			    
		} else { // send exit ack to master
			SendPort mp = null;

			synchronized(this) {
				mp = getReplyPortWait(masterIdent);
			}

			try {
				WriteMessage writeMessage;
				if(COMM_DEBUG) {
					out.println("SATIN '" + ident.name() + 
						    "': sending exit ACK message to " + masterIdent);
				}
				
				writeMessage = mp.newMessage();
				writeMessage.writeByte(Protocol.EXIT_REPLY);
				if(stats) {
					writeMessage.writeObject(createStats());
				}
				writeMessage.finish();
			} catch (IOException e) {
				synchronized(this) {
					System.err.println("SATIN: Could not send exit message to " + masterIdent);
				}
			}
			algorithm.exit(); //give the algorithm time to clean up
		}
		
		

		barrier(); // Wait until everybody agrees to exit. 

		if(master && stats) printStats();
		
//		System.exit(1);

		try {
			if(SUPPORT_TUPLE_MULTICAST) {
				tuplePort.close();
			}
		} catch (Throwable e) {
			System.err.println("tuplePort.close() throws " + e);
		}

		// If not closed, free ports. Otherwise, ports will be freed in leave calls.
		while(true) {
			try {
				SendPort s;
			
				synchronized(this) {
					if(victims.size() == 0) break;

					s = victims.getPort(0);
					
					if(COMM_DEBUG) {
						out.println("SATIN '" + ident.name() + 
							    "': freeing sendport to " +
							    victims.getIdent(0));
					}
					victims.remove(0);
				}
			
				if(s != null) {
					s.close();
				}
			
//				if(COMM_DEBUG) {
//				  out.println(" DONE");
//				  }
			} catch (Throwable e) {
				System.err.println("port.close() throws " + e);
			}
		}
		
		try {
			receivePort.close();

			if(master) {
				barrierReceivePort.close();
			} else {
				barrierSendPort.close();
			}
		} catch (Throwable e) {
			System.err.println("port.close() throws " + e);
		}
		
		if (FAULT_TOLERANCE) {
		    globalResultTable.exit();
		}
		

		try {
		    ibis.end();
		} catch (Throwable e) {
			System.err.println("ibis.end throws " + e);
		}
			
		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': exited");
		}

		// Do a gc, and run the finalizers. Useful for printing statistics in Satin applications.
		// The app should register a shutdownhook. --Rob
		System.gc();
		System.runFinalization();
		System.runFinalizersOnExit(true);

		System.exit(0); // Needed for IBM jit.
	}

	/* Only allowed when not stealing. */
	private void barrier() {
		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': barrier start");
		}

		if(!closed) {
			ibis.closeWorld();
		}

		int size;
		synchronized(this) {
			size = victims.size();
		}

		try {
			if(master) {
				for(int i=0; i<size; i++) {
					ReadMessage r = barrierReceivePort.receive();
					r.finish();
				}

				for(int i=0; i<size; i++) {
					SendPort s;
					synchronized(this) {
						s = victims.getPort(i);
					}
					WriteMessage writeMessage = s.newMessage();
					writeMessage.writeByte(Protocol.BARRIER_REPLY);
					writeMessage.finish();
				}
			} else {
				WriteMessage writeMessage = barrierSendPort.newMessage();
				writeMessage.finish();
				
				if (!upcalls) {
					while(!gotBarrierReply/* && !exiting */) {
						satinPoll();
					}
					/* Imediately reset gotBarrierReply, we know that a reply has arrived. */
					gotBarrierReply = false; 
				} else {
					synchronized(this) {
						while(!gotBarrierReply) {
							try {
								wait();
							} catch (InterruptedException e) {
				// Ignore.
							}
						}
						/* Imediately reset gotBarrierReply, we know that a reply has arrived. */
						gotBarrierReply = false; 
					}
				}
			}
		} catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + 
					   "': error in barrier");
			System.exit(1);
		}

		if(!closed) {
			ibis.openWorld();
		}

		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': barrier DONE");
		}
	}

	// hold the lock when calling this
	protected void addToOutstandingJobList(InvocationRecord r) {
		if(ASSERTS) {
			assertLocked(this);
		}
		outstandingJobs.add(r);
	}

	// hold the lock when calling this
	protected void addToActiveTupleList(String key, Serializable data) {
		if(ASSERTS) {
			assertLocked(this);
		}
		activeTupleKeyList.add(key);
		activeTupleDataList.add(data);
	}

	// hold the lock when calling this
	protected void addToJobResultList(InvocationRecord r) {
		if(ASSERTS) {
			assertLocked(this);
		}
		resultList.add(r);
	}

	// hold the lock when calling this
	protected InvocationRecord getStolenInvocationRecord(int stamp, SendPortIdentifier sender, IbisIdentifier owner) {
		if(ASSERTS) {
			assertLocked(this);
			if(owner == null) { 
				System.err.println("SATIN '" + ident.name() + 
						   "': owner is null in getStolenInvocationRecord");
				System.exit(1);
			}
			if(!owner.equals(ident)) {
				System.err.println("SATIN '" + ident.name() + 
						   "': Removing wrong stamp!");
				System.exit(1);
			}
		}
		return outstandingJobs.remove(stamp, owner);
	}

	protected void sendResult(InvocationRecord r, ReturnRecord rr) {
	    if(/*exiting ||*/ r.alreadySentExceptionResult) return;

		if(ASSERTS && r.owner == null) {
			System.err.println("SATIN '" + ident.name() + 
					   "': owner is null in sendResult");
			System.exit(1);
		}

		if(STEAL_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': sending job result to " +
				    r.owner.name() + ", exception = " + (r.eek == null ? "null" : ("" + r.eek)));
		}

		SendPort s = null;
		
/*		boolean noUpdate = false;
		if (NUM_CRASHES > 0) {		    
		    for (int i=1; i<NUM_CRASHES+1 && i<allIbises.size(); i++) {
			IbisIdentifier id = (IbisIdentifier) allIbises.get(i);
			if (id.equals(ident)) {
//			    System.out.println(ident.name() + " - i'm not sending the result");
			    noUpdate = true;
			}
		    }
		}*/
		
		synchronized (this) {
		
/*		    if (FAULT_TOLERANCE) {
			if (GRT_TIMING) {
			    updateTimer.start();
			}
			globalResultTable.updateInvocationRecord(r);
			
			if (GRT_TIMING) {
			    updateTimer.stop();
			}
		    }*/
		    s = getReplyPortNoWait(r.owner);
		}
		
		if (s == null) {
		    //probably crashed..
		    return;
		}			
		

		try {
			WriteMessage writeMessage = s.newMessage();
			if(r.eek == null) {
				writeMessage.writeByte(Protocol.JOB_RESULT_NORMAL);
				writeMessage.writeObject(r.owner);  // hmm, I don't think this is needed --Rob @@@
				writeMessage.writeObject(rr);
			} else {
				if (rr == null) r.alreadySentExceptionResult = true;
				writeMessage.writeByte(Protocol.JOB_RESULT_EXCEPTION);
				writeMessage.writeObject(r.owner);  // hmm, I don't think this is needed --Rob @@@
				writeMessage.writeObject(r.eek);
				writeMessage.writeInt(r.stamp);
			}
			long cnt = writeMessage.finish();

			if(STEAL_STATS) {
				if(inDifferentCluster(r.owner)) {
					interClusterMessages++;
					interClusterBytes += cnt;
				} else {
					intraClusterMessages++;
					intraClusterBytes += cnt;
				}
			} 
		} catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + 
					   "': Got Exception while sending steal request: " + e);
			System.exit(1);
		}
	}

	/* does a synchronous steal */
	protected void stealJob(Victim v) {

		if(ASSERTS && stolenJob != null) {
			throw new IbisError("EEEK, trying to steal while an unhandled stolen job is available.");
		}
		if(STEAL_TIMING) {
			stealTimer.start();
		}

		if(STEAL_STATS) {
			stealAttempts++;
		}

		sendStealRequest(v, true, false);
		waitForStealReply();
	}

	/* does a synchronous steal, but blocks on server side until work is available, or we must exit */
	protected void stealJobBlocking(Victim v) {

		if(ASSERTS && stolenJob != null) {
			throw new IbisError("EEEK, trying to steal while an unhandled stolen job is available.");
		}
/*
  synchronized(this) {
  q.print(System.err);
  outstandingJobs.print(System.err);
  onStack.print(System.err);
  }
*/
		if(STEAL_TIMING) {
			stealTimer.start();
		}

		if(STEAL_STATS) {
			stealAttempts++;
		}

		sendStealRequest(v, true, true);
		waitForStealReply();
	}

	protected void sendStealRequest(Victim v, boolean synchronous, boolean blocking) {
		if(exiting) return;

		if(STEAL_DEBUG && synchronous) {
			System.err.println("SATIN '" + ident.name() + 
					   "': sending steal message to " +
					   v.ident.name());
		}
		if(STEAL_DEBUG && !synchronous) {
			System.err.println("SATIN '" + ident.name() + 
					   "': sending ASYNC steal message to " +
					   v.ident.name());
		}

		try {
			SendPort s = v.s;
			WriteMessage writeMessage = s.newMessage();
			byte opcode = -1;

			if(synchronous) {
				if(blocking) {
					opcode = Protocol.BLOCKING_STEAL_REQUEST;
				} else {
					if (FAULT_TOLERANCE) {
					    synchronized (this) {
						if (getTable) {
						    opcode = Protocol.STEAL_AND_TABLE_REQUEST;
						} else {
						    opcode = Protocol.STEAL_REQUEST;
						}
					    }
					} else {					     
					    opcode = Protocol.STEAL_REQUEST;
					}
				}
			} else {
				if (FAULT_TOLERANCE) {
				    synchronized (this) {
					if (clusterCoordinator && getTable) {
					    opcode = Protocol.ASYNC_STEAL_AND_TABLE_REQUEST;
					} else {
					    if (getTable) {
						System.err.println("SATIN '" + ident.name() +
								   ": EEEK sending async steal message while waiting for table!!");
					    }
					    opcode = Protocol.ASYNC_STEAL_REQUEST;
					}
				    }
				} else {
				    opcode = Protocol.ASYNC_STEAL_REQUEST;
				}
			}

			writeMessage.writeByte(opcode);
			long cnt = writeMessage.finish();
			if(STEAL_STATS) {
				if(inDifferentCluster(v.ident)) {
					interClusterMessages++;
					interClusterBytes += cnt;
				} else {
					intraClusterMessages++;
					intraClusterBytes += cnt;
				}
			}
		} catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + 
					   "': Got Exception while sending " +
					   (synchronous ? "" : "a") + "synchronous" +
					   " steal request: " + e);
			System.exit(1);
		}
	}

	protected boolean waitForStealReply() {
//		if(exiting) return false;

		if(IDLE_TIMING) {
			idleTimer.start();
		}

		// Replaced this wait call, do something useful instead:
		// handleExceptions and aborts.
		if(upcalls) {
			if(HANDLE_MESSAGES_IN_LATENCY) {
				while(true) {
					satinPoll();

					if(ABORTS || FAULT_TOLERANCE) {
						handleDelayedMessages();
					}

					synchronized(this) {

						if(gotStealReply) {
							/* Immediately reset gotStealReply, we know that
							   a reply has arrived. */
							gotStealReply = false;
							currentVictimCrashed = false;
							break;
						}
						
						if (FAULT_TOLERANCE) {
						    if(currentVictimCrashed) {						    
							currentVictimCrashed = false;
							if (gotStealReply == false) return false;
						    }
						}
					}
//					Thread.yield();
				}
			} else {
				synchronized(this) {
					while(!gotStealReply) {
						
						if (FAULT_TOLERANCE) {						
						    if(currentVictimCrashed) {
							currentVictimCrashed = false;
//							System.err.println("SATIN '" + ident.name() + "': current victim crashed");
							if (gotStealReply == false) return false;
							break;							
						    }
						}


						try {
							wait();
						} catch (InterruptedException e) {
							throw new IbisError(e);
						}
						
					}
					/* Immediately reset gotStealReply, we know that a
					   reply has arrived. */
					gotStealReply = false;
				}
			}
		} else { // poll for reply
			while(!gotStealReply) {
				satinPoll();
				if (FAULT_TOLERANCE) {				    
				    if(currentVictimCrashed) {
					currentVictimCrashed = false;
					if (gotStealReply == false) return false;
				    }
				}
				
			}
			gotStealReply = false;
		}

		if(IDLE_TIMING) {
			idleTimer.stop();
		}

		if(STEAL_TIMING) {
			stealTimer.stop();
		}

		/*if(STEAL_DEBUG) {
		  out.println("SATIN '" + ident.name() + 
		  "': got synchronous steal reply: " +
		  (stolenJob == null ? "FAILED" : "SUCCESS"));
		  }*/

		/* If successfull, we now have a job in stolenJob. */
		if (stolenJob == null) {
			return false;
		}

		/* I love it when a plan comes together! */

		if(STEAL_STATS) {
			stealSuccess++;
		}

		InvocationRecord myJob = stolenJob;
		stolenJob = null;

		stolenFrom = myJob.owner;

		// if we have ordered communication, we have to wait until
		// our sequence number equals the one in the job
		if(sequencer != null) {
		    if(TUPLE_TIMING) {
			tupleOrderingWaitTimer.start();
		    }
		    if(TUPLE_DEBUG) {
			System.err.println("steal reply seq nr = " + stealReplySeqNr + ", my seq nr = " + expected_seqno);
		    }
		    while(stealReplySeqNr > expected_seqno) {
			handleDelayedMessages();
		    }
		    if(TUPLE_TIMING) {
			tupleOrderingWaitTimer.stop();
		    }
		}

		callSatinFunction(myJob);

		return true;
	}


	private void handleJoin(IbisIdentifier joiner) {
		try {
			ReceivePortIdentifier r = null;
			SendPort s = portType.createSendPort("satin sendport");

			r = lookup("satin port on " + joiner.name());
			
			if (FAULT_TOLERANCE) {
			    if (!connect(s, r, connectTimeout)) {
				if (COMM_DEBUG) {
				    out.println("SATIN '" + ident.name() + "': unable to connect to " + joiner.name() + ", might have crashed");
				}
				return;
			    }
			} else {
			    connect(s, r);
			}
			
			if (SUPPORT_TUPLE_MULTICAST) {
				connect(tuplePort,r);
			}

			synchronized (Satin.this) {
				if (FAULT_TOLERANCE) {
				    globalResultTable.addReplica(joiner);
				}			
				victims.add(joiner, s);
				Satin.this.notifyAll();				
			}
						
			if(COMM_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': " + joiner.name() + " JOINED");
			}
		} catch (Exception e) {
			System.err.println("SATIN '" + ident + 
					   "': got an exception in Satin.join: " + e);
			e.printStackTrace(System.err);
			System.exit(1);
		}
//		}
	}


	public void join(IbisIdentifier joiner) {
		
		if(joiner.name().equals("ControlCentreIbis")) return;
//		allIbises.add(joiner);
		if(joiner.equals(ident)) return;

		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': '" + joiner.name() + "' from cluster '" +
				    joiner.cluster() + "' is trying to join");
		}		
//		if (!victims.contains(joiner)) {		
		handleJoin(joiner);
	}

	public void leave(IbisIdentifier leaver) {		
		if(leaver.equals(this.ident)) return;

		if(COMM_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': " + leaver.name() + " left");
		}

		Victim v;
		
		

		synchronized (this) {
			if (FAULT_TOLERANCE) {
			    globalResultTable.removeReplica(leaver);
			}
			v = victims.remove(leaver);
			notifyAll();

			if (v != null && v.s != null) {
				try {
					v.s.close();
				} catch (IOException e) {
					System.err.println("port.close() throws " + e);
				}
			}
		}
	}
	
	
	public void delete(IbisIdentifier id) {
//	    System.out.println("SATIN '" + ident.name() + "': got delete " + id.address());
	
	    if (ident.equals(id)) {
		gotDelete = true;
	    }
	
	}
	
	public void reconfigure() {
	}

	//used by GlobalResultTable
	static void connect(SendPort s, ReceivePortIdentifier ident) {
		boolean success = false;
		do {
			try {
				s.connect(ident);
				success = true;
			} catch (IOException e) {
				try {
					Thread.sleep(500);
				} catch (InterruptedException e2) {
					// ignore
				}
			}
		} while (!success);
	}
	
	static boolean connect(SendPort s, ReceivePortIdentifier ident, long timeoutMillis) {
		boolean success = false;
		long startTime = System.currentTimeMillis();
		do {
			try {
				s.connect(ident, timeoutMillis);
				success = true;
			} catch (IOException e) {
				try {
					Thread.sleep(500);
				} catch (InterruptedException e2) {
					// ignore
				}
			}
		} while (!success && System.currentTimeMillis() - startTime < timeoutMillis);
		return success;
	}
	

	//used by GlobalResultTable
	ReceivePortIdentifier lookup(String name) throws IOException { 
		ReceivePortIdentifier temp = null;
		do {
			temp = ibis.registry().lookup(name);

			if (temp == null) {
				try {
					//					System.err.print("."); System.err.flush();
					Thread.sleep(500);
				} catch (InterruptedException e) {
					// ignore
				}
			}
			
		} while (temp == null);
				
		return temp;
	} 

	void addToExceptionList(InvocationRecord r) {
		if(ASSERTS) {
			assertLocked(this);
		}
		exceptionList.add(r);
		gotExceptions = true;
		if(INLET_DEBUG) {
			out.println("SATIN '" + ident.name() + ": got remote exception!");
		}
	}

	void addToAbortList(int stamp, IbisIdentifier owner) {
		if(ASSERTS) {
			assertLocked(this);
		}
		if(ABORT_DEBUG) {
			out.println("SATIN '" + ident.name() + ": got abort message");
		}
		abortList.add(stamp, owner);
		gotAborts = true;
	}

	// Used for fault tolerance	
	void addToAbortAndStoreList(int stamp, IbisIdentifier owner) {
		if(ASSERTS) {
			assertLocked(this);
		}
		if(ABORT_DEBUG) {
			out.println("SATIN '" + ident.name() + ": got abort message");
		}
		abortAndStoreList.add(stamp, owner);
		gotAbortsAndStores = true;
	}
	
	SendPort getReplyPortWait(IbisIdentifier ident) {
		SendPort s;
		if(ASSERTS) {
			assertLocked(this);
		}
		
		do {
			s = victims.getReplyPort(ident);
			if(s == null) {
				if(COMM_DEBUG) {
					
					out.println("SATIN '" + this.ident.name() + 
						    "': could not get reply port to " +
						    ident.name() + ", retrying");
				}
				try {
					wait();
				} catch (Exception e) {
					// Ignore.
				}
			}
		} while (s == null);

		return s;
	}
	
	SendPort getReplyPortNoWait(IbisIdentifier ident) {
		SendPort s;
		if (ASSERTS) {
			assertLocked(this);
		}
		s = victims.getReplyPort(ident);
		return s;
	}


	/* message combining for abort messages does not work (I tried). It is very unlikely that
	   one node stole more than one job from me */
	void sendAbortMessage(InvocationRecord r) {
		if(ABORT_DEBUG) {
			out.println("SATIN '" + ident.name() + ": sending abort message to: " + 
				    r.stealer + " for job " + r.stamp);
		}

		if (deadIbises.contains(r.stealer)) {
		    /*don't send abort and store messages to crashed ibises*/
		    return;
		}


		try {
			SendPort s = getReplyPortNoWait(r.stealer);
			if (s == null) return;
			
			WriteMessage writeMessage = s.newMessage();
			writeMessage.writeByte(Protocol.ABORT);
			writeMessage.writeInt(r.parentStamp);
			writeMessage.writeObject(r.parentOwner);
			long cnt = writeMessage.finish();
			if(STEAL_STATS) {
				if(inDifferentCluster(r.stealer)) {
					interClusterMessages++;
					interClusterBytes += cnt;
				} else {
					intraClusterMessages++;
					intraClusterBytes += cnt;
				}
			} 
		} catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + 
					   "': Got Exception while sending abort message: " + e);
			// This should not be a real problem, it is just inefficient.
			// Let's continue...
			// System.exit(1);
		}
	}

	// Used for fault tolerance
	void sendAbortAndStoreMessage(InvocationRecord r) {
		if(ABORT_DEBUG) {
			out.println("SATIN '" + ident.name() + ": sending abort and store message to: " + 
					   r.stealer + " for job " + r.stamp);
		}
		
		if (deadIbises.contains(r.stealer)) {
		    /*don't send abort and store messages to crashed ibises*/
		    return;
		}
		
		try {
			SendPort s = getReplyPortNoWait(r.stealer);
			if (s == null) return;
			
			WriteMessage writeMessage = s.newMessage();
			writeMessage.writeByte(Protocol.ABORT_AND_STORE);
			writeMessage.writeInt(r.parentStamp);
			writeMessage.writeObject(r.parentOwner);
			writeMessage.send();
			long cnt = writeMessage.finish();
			if(STEAL_STATS) {
				if(inDifferentCluster(r.stealer)) {
					interClusterMessages++;
					interClusterBytes += cnt;
				} else {
					intraClusterMessages++;
					intraClusterBytes += cnt;
				}
			} 
		} catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + 
						   "': Got Exception while sending abort message: " + e);
			// This should not be a real problem, it is just inefficient.
			// Let's continue...
			// System.exit(1);
		}
	}


	boolean satinPoll() {
		if(POLL_FREQ == 0) { // polling is disabled
		    if(HANDLE_MESSAGES_IN_LATENCY) {
			System.err.println("Polling is disabled while messages are handled in the latency.\n" + 
					   "This is a configureation error.");
			System.exit(1);
		    }
			return false;
		} 

		if(upcalls && !upcallPolling) { // we are using upcalls, but don't want to poll
		    if(HANDLE_MESSAGES_IN_LATENCY) {
			System.err.println("Polling is disabled while messages are handled in the latency.\n" + 
					   "This is a configureation error.");
			System.exit(1);
		    }
			return false;
		}

		if (POLL_FREQ > 0) {
			long curr = pollTimer.currentTimeNanos();
			if(curr - prevPoll < POLL_FREQ) {
				return false;
			}
			prevPoll = curr;
		}

		if(POLL_TIMING) pollTimer.start();

		ReadMessage m = null;
		if(POLL_RECEIVEPORT) {
			try {
				m = receivePort.poll();
			} catch (IOException e) {
				System.err.println("SATIN '" + ident.name() + 
						   "': Got Exception while polling: " + e);
			}

			if(m != null) {
				messageHandler.upcall(m);
				try {
					m.finish(); // Finish the message, the upcall does not need to do this.
				} catch (Exception e) {
					System.err.println("error in finish: " + e);
				}
			}
		} else {
			try {
				ibis.poll(); // does not return message, but triggers upcall.
			} catch (Exception e) {
				System.err.println("polling failed, continuing anyway");
			}
		}

		if(POLL_TIMING) pollTimer.stop();

		return m != null;
	}

	private void handleDelayedMessages() {
		if (ABORTS) { 
			if(gotAborts) handleAborts();
			if(gotExceptions) handleExceptions();
		}
		if(receivedResults) handleResults();
		if(gotActiveTuples) handleActiveTuples();
		
		if (FAULT_TOLERANCE) {				
			if(gotCrashes) handleCrashes();
			if(gotAbortsAndStores) handleAbortsAndStores();
			if(gotDelete) handleDelete();
		}
		
	}

	/**
	 * Obtains a new spawn counter.
	 * This does not need to be synchronized, only one thread spawns.
	 * @return a new spawn counter.
	 */
	static public SpawnCounter newSpawnCounter() {
		if(spawnCounterCache == null) {
			return new SpawnCounter();
		}

		SpawnCounter res = spawnCounterCache;
		spawnCounterCache = res.next;
		res.value = 0;

		return res;
	}

	/**
	 * Makes a spawn counter available for recycling.
	 * This does not need to be synchronized, only one thread spawns.
	 * @param s the spawn counter made available.
	 */
	static public void deleteSpawnCounter(SpawnCounter s) {
		if(ASSERTS && s.value < 0) {
			System.err.println("deleteSpawnCounter: spawncouner < 0, val =" + s.value);
			new Exception().printStackTrace();
			System.exit(1);
		}

		s.next = spawnCounterCache;
		spawnCounterCache = s;
	}


	synchronized void addJobResult(ReturnRecord rr, SendPortIdentifier sender, IbisIdentifier i, 
				       Throwable eek, int stamp) {
		receivedResults = true;
		InvocationRecord r = null;

		if (rr != null) {
			r = getStolenInvocationRecord(rr.stamp, sender, i);
		} else {
			r = getStolenInvocationRecord(stamp, sender, i);
		}

		if(r != null) {
			if(rr != null) {
				rr.assignTo(r);
			} else {
				r.eek = eek;
			}
			if(r.eek != null) { // we have an exception, add it to the list. the list will be read during the sync
				if(ABORTS) {
					addToExceptionList(r);
				} else {
					throw new IbisError("Got exception result", r.eek);
				}
			} else {
				addToJobResultList(r);
			}
		} else {
			if(ABORTS || FAULT_TOLERANCE) {
				if (ABORT_DEBUG) {
					out.println("SATIN '" + ident.name() + 
						    "': got result for aborted job, ignoring.");
				}
			} else {
				out.println("SATIN '" + ident.name() + 
					    "': got result for unknown job!");
				System.exit(1);
			}
		}
	}

	private void handleInlet(InvocationRecord r) {
		InvocationRecord oldParent;
		int oldParentStamp;
		IbisIdentifier oldParentOwner;

		if(r.inletExecuted) {
		    System.err.print("r");
		    return;
		}

		if(r.parentLocals == null) {
		    System.err.println("empty inlet in handleInlet");
		    handleEmptyInlet(r);
		    return;
		}

		onStack.push(r);
		oldParent = parent;
		oldParentStamp = parentStamp;
		oldParentOwner = parentOwner;
		parentStamp = r.stamp;
		parentOwner = r.owner;
		parent = r;

		try {
			if(INLET_DEBUG) {
				System.err.println("SATIN '" + ident.name() + ": calling inlet caused by remote exception");
			}

			r.parentLocals.handleException(r.spawnId, r.eek, r);
			r.inletExecuted = true;

			// restore these, there may be more spawns afterwards...
			parentStamp = oldParentStamp;
			parentOwner = oldParentOwner;
			parent = oldParent;
			onStack.pop();

		} catch (Throwable t) {
		    // The inlet has thrown an exception itself.
		    // The semantics of this: throw the exception to the parent,
		    // And execute the inlet if it has one (might be an empty one).
		    // Also, the other children of the parent must be aborted.
			r.inletExecuted = true;

			// restore these, there may be more spawns afterwards...
			parentStamp = oldParentStamp;
			parentOwner = oldParentOwner;
			parent = oldParent;
			onStack.pop();

			if(INLET_DEBUG) {
				System.err.println("Got an exception from exception handler! " + t);
//						t.printStackTrace();
				System.err.println("r = " + r);
				System.err.println("r.parent = " + r.parent);
			}
			if(r.parent == null) {
				System.err.println("An inlet threw an exception, but there is no parent that handles it." + t);
				t.printStackTrace();
				System.exit(1);
			}

			if(ABORT_STATS) {
				aborts++;
			}

			synchronized(this) {
				// also kill the parent itself.
				// It is either on the stack or on a remote machine.
				// Here, this is OK, the child threw an exception, 
				// the parent did not catch it, and must therefore die.
				r.parent.aborted = true;
				r.parent.eek = t; // rethrow exception
				killChildrenOf(r.parent.stamp, r.parent.owner);
			}

			if(!r.parentOwner.equals(ident)) {
			    if(INLET_DEBUG || STEAL_DEBUG) {
				System.err.println("SATIN '" + ident.name() + ": prematurely sending exception result");
			    }
			    sendResult(r.parent, null);
			    return;
			}

			// two cases here: empty inlet or normal inlet
			if(r.parent.parentLocals == null) { // empty inlet
			    handleEmptyInlet(r.parent);
			} else { // normal inlet
			    handleInlet(r.parent);
			}
		}
        }

/*
        // trace back from the exception, and execute inlets / empty imlets back to the root
        // during this, prematurely send result messages.
	private void handleEmptyInlet(InvocationRecord r) {
		// if r does not have parentLocals, this means
		// that the PARENT does not have a try catch block around the spawn.
		// there is thus no inlet to call in the parent.

		if(r.parentLocals != null || r.eek == null) return;

		if(INLET_DEBUG) {
			out.println("SATIN '" + ident.name() + ": Got exception, empty inlet: " + r.eek + 
				    ": " + r.eek.getMessage());
//				r.eek.printStackTrace();
		}

		InvocationRecord curr = r;

		synchronized(this) {
			while(curr.parentLocals == null && curr.parent != null) {
				if(INLET_DEBUG) {
					System.err.println("SATIN '" + ident.name() + ": unwind");
				}
				curr = curr.parent;
				
				if(SPAWN_STATS) {
					aborts++;
				}
			}
 
			if(INLET_DEBUG) {
				System.err.println("SATIN '" + ident.name() + ": unwind stopped, curr = " + curr);
			}

			// also kill the parent itself.
			// It is either on the stack or on a remote machine.
			// Here, this is OK, the child threw an exception, 
			// the parent did not catch it, and must therefore die.
			curr.aborted = true;
			curr.eek = r.eek; // rethrow exception
			killChildrenOf(curr.stamp, curr.owner);

			if(curr.parentLocals != null) { // parent has inlet
				handleInlet(curr);
			}

			if(!curr.parentOwner.equals(ident)) {
				if(INLET_DEBUG || STEAL_DEBUG) {
					System.err.println("SATIN '" + ident.name() + ": prematurely sending exception result");
				}
				sendResult(curr, null);
			}
		}
	}
*/

        // trace back from the exception, and execute inlets / empty imlets back to the root
        // during this, prematurely send result messages.
	private void handleEmptyInlet(InvocationRecord r) {
		// if r does not have parentLocals, this means
		// that the PARENT does not have a try catch block around the spawn.
		// there is thus no inlet to call in the parent.

		if(r.eek == null) return;
		if(r.parent == null) return;
//		if(r.parenLocals != null) return;

		if(ASSERTS && r.parentLocals != null) {
		    System.err.println("parenlocals is not null in empty inlet");
		    System.exit(1);
		}

//		if(INLET_DEBUG) {
			out.println("SATIN '" + ident.name() + ": Got exception, empty inlet: " + r.eek + 
				    ": " + r.eek.getMessage());
//				r.eek.printStackTrace();
//		}

		InvocationRecord curr = r;

		synchronized(this) {
			// also kill the parent itself.
			// It is either on the stack or on a remote machine.
			// Here, this is OK, the child threw an exception, 
			// the parent did not catch it, and must therefore die.
			r.parent.aborted = true;
			r.parent.eek = r.eek; // rethrow exception
			killChildrenOf(r.parent.stamp, r.parent.owner);
		}
		
		if(!r.parentOwner.equals(ident)) {
		    if(INLET_DEBUG || STEAL_DEBUG) {
			System.err.println("SATIN '" + ident.name() + ": prematurely sending exception result");
		    }
		    sendResult(r.parent, null);
		    return;
		}

		// now the recursion step
		if(r.parent.parentLocals != null) { // parent has inlet
		    handleInlet(r.parent);
		} else {
		    handleEmptyInlet(r.parent);
		}
	}

	private synchronized void handleResults() {
		while (true) {
			InvocationRecord r = resultList.removeIndex(0);
			if(r == null) break;

			if(r.eek != null) {
			    handleInlet(r);
			}

			r.spawnCounter.value--;
			
			if (FAULT_TOLERANCE) {
			    //add the finished job to children list
			    if (r.parent != null) {
				r.sibling = r.parent.child;
				r.parent.child = r;
			    }
			}
			if(ASSERTS && r.spawnCounter.value < 0) {
			    out.println("Just made spawncounter < 0");
			    new Exception().printStackTrace();
			    System.exit(1);
			}
		}

		receivedResults = false;
	}

	protected void callSatinFunction(InvocationRecord r) {
		InvocationRecord oldParent;
		int oldParentStamp;
		IbisIdentifier oldParentOwner;

		handleDelayedMessages();

		if(ABORTS || FAULT_TOLERANCE) {
			oldParent = parent;
			oldParentStamp = parentStamp;
			oldParentOwner = parentOwner;
		}

		if(ASSERTS) {
			if(r == null) {
				out.println("SATIN '" + ident.name() +
					    ": EEK, r = null in callSatinFunc");
				System.exit(1);
			}
			if(r.aborted) {
				out.println("SATIN '" + ident.name() +
					    ": spawning aborted job!");
				System.exit(1);
			}

			if(r.owner == null) {
				out.println("SATIN '" + ident.name() +
					    ": EEK, r.owner = null in callSatinFunc, r = " + r);
				new Throwable().printStackTrace();
				System.exit(1);
			}

			if(r.owner.equals(ident)) {
				if(r.spawnCounter.value < 0) {
					out.println("SATIN '" + ident.name() + 
						    ": spawncounter < 0 in callSatinFunc");
					System.exit(1);
				}
		
				if(ABORTS && r.parent == null && parentOwner.equals(ident) &&
				   r.parentStamp != -1) { 
					out.println("SATIN '" + ident.name() +
						    ": parent is null for non-root, should not happen here! job = " + r);
					System.exit(1);
				}
			}
		}

		if((ABORTS || FAULT_TOLERANCE) && r.parent != null && r.parent.aborted) {
			if(ABORT_DEBUG) { 
				out.print("SATIN '" + ident.name());
				out.print(": spawning job, parent was aborted! job = " + r);
				out.println(", parent = " + r.parent + "\n");
			}
			r.spawnCounter.value--;
			if(ASSERTS) {
				if(r.spawnCounter.value < 0) {
					out.println("SATIN '" + ident.name() + 
						    ": Just made spawncounter < 0");
					new Exception().printStackTrace();
					System.exit(1);
				}
			}
			return;
		}
	
		if(ABORTS || FAULT_TOLERANCE) {
			onStack.push(r);
			parent = r;
			parentStamp = r.stamp;
			parentOwner = r.owner;
		}

		if(SPAWN_DEBUG) {
			out.println("SATIN '" + ident.name() +
				    "': callSatinFunc: stamp = " + r.stamp +
				    ", owner = " +
				    (r.owner.equals(ident) ? "me" : r.owner.toString()) +
				    ", parentStamp = " + r.parentStamp +
				    ", parentOwner = " + r.parentOwner);
		}

		if(r.owner.equals(ident)) {
			if (SPAWN_DEBUG) {
				out.println("SATIN '" + ident.name() +
					    "': callSatinFunc: spawn counter = " +
					    r.spawnCounter.value);
			}
			if(ABORTS) {
				if(SPAWN_STATS) {
					jobsExecuted++;
				}
				try {
					r.runLocal();
				} catch (Throwable t) { 
                                        // This can only happen if an inlet has thrown an exception.
					// The semantics of this: all work is aborted,
					// and the exception is passed on to the spawner.
					// The parent is aborted, it must handle the exception.

				    r.eek = t;
				    handleInlet(r);
				}
			} else { // NO aborts
				if(SPAWN_STATS) {
					jobsExecuted++;
				}
				try {
				    r.runLocal();
				} catch (Throwable t) {
				    throw new IbisError("Unexpected exception in runLocal", t);
				}
			}

			r.spawnCounter.value--;
			if(ASSERTS && r.spawnCounter.value < 0) {
			    out.println("SATIN '" + ident.name() + ": Just made spawncounter < 0");
			    new Exception().printStackTrace();
			    System.exit(1);
			}

			if(ASSERTS && !ABORTS && r.eek != null) {
				out.println("Got exception: " + r.eek);
				System.exit(1);
			}

			if(SPAWN_DEBUG) {
				out.print("SATIN '" + ident.name() + ": callSatinFunc: stamp = " + r.stamp + 
					  ", parentStamp = " + r.parentStamp + 
					  ", parentOwner = " + r.parentOwner + " spawn counter = " + r.spawnCounter.value);

				if(r.eek == null) {
					out.println(" DONE");
				} else {
					out.println(" DONE with exception: " + r.eek);
				}
			}
			if (FAULT_TOLERANCE) {
			    //add the finished job to children list
			    if (r.parent != null && !r.aborted) {
	    			r.sibling = r.parent.child;
				r.parent.child = r;
			    }
			}
			
		} else {
			if(STEAL_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': RUNNING REMOTE CODE!");
			}
			ReturnRecord rr = null;
			if(ABORTS) {
				if(SPAWN_STATS) {
					jobsExecuted++;
				}
				try {
					rr = r.runRemote();
					// May be needed if the method did not throw an exception,
					// but its child did, and there is an empty inlet.
					rr.eek = r.eek; 
				} catch (Throwable t) {
					out.println("SATIN '" + ident.name() + ": OOOhh dear, got exception in runremote: " + t);
					t.printStackTrace();
					System.exit(1);
				}
			} else {
				if(SPAWN_STATS) {
					jobsExecuted++;
				}
				rr = r.runRemote();
			}
			if(STEAL_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': RUNNING REMOTE CODE DONE!");
			}

			if(STEAL_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': REMOTE CODE SEND RESULT!, exception = " + (r.eek == null ? "null" : ("" + r.eek)));
			}
			// send wrapper back to the owner
			if (!r.aborted) {
			    sendResult(r, rr);
			}

			if(STEAL_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': REMOTE CODE SEND RESULT DONE!");
			}
		}
		
		if (ABORTS || FAULT_TOLERANCE) {
			// restore these, there may be more spawns afterwards...
			parentStamp = oldParentStamp;
			parentOwner = oldParentOwner;
			parent = oldParent;
			onStack.pop();
		}
		
		if (FAULT_TOLERANCE) {
		    //remove its children list, will anyone ever look at it?
		    r.child = null;
		}

		if(ABORT_DEBUG && r.aborted) {
			System.err.println("Job on the stack was aborted: " + r.stamp + " EEK = " + (r.eek == null ? "null" : ("" + r.eek)));
		}

		if(SPAWN_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': call satin func done!");
		}
	}

	/**
	 * Implements the main client loop: steal jobs and execute them.
	 */
	public void client() {
		InvocationRecord r;
		SendPort s;
		
//		System.err.println("SATIN " + ident.name() + ": starting client()");

		if(SPAWN_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': starting client!");
		}

		while(!exiting) {
			// steal and run jobs

		        satinPoll();
			handleDelayedMessages();

			algorithm.clientIteration();
			
			//for ft
			if (master) return;
		}
		
//		System.err.println("SATIN " + ident.name() + ": client() done");
	}

	/**
	 * Spawns the method invocation as described by the specified invocation
	 * record. The invocation record is added to the job queue maintained
	 * by this Satin.
	 * @param r the invocation record specifying the spawned invocation.
	 */
	public void spawn(InvocationRecord r) {
		if(ASSERTS) {
			if(algorithm instanceof MasterWorker) {
				synchronized(this) {
					if(!ident.equals(masterIdent)) {
						System.err.println("with the master/worker algorithm, work can only be spawned on the master!");
						System.exit(1);
					}
				}
			}
		}

		if(SPAWN_STATS) {
			spawns++;
		}

		r.spawnCounter.value++; 
				
		if (GLOBALLY_UNIQUE_STAMPS) {
		    //globally unique stamps start from 1 (root job)
		    if (branchingFactor > 0) {
			
			if (parentStamp > 0) {
			    r.stamp = branchingFactor * parentStamp + parent.numSpawned++;
			} else {
			    //parent is the root
			    r.stamp = branchingFactor + rootNumSpawned++;
			}
		    } else {
			r.stamp = stampCounter++;
		    }
		} else {    
		    r.stamp = stampCounter++;
		}
		
		
		r.owner = ident;

		if(ABORTS || FAULT_TOLERANCE) {
			r.parentStamp = parentStamp;
			r.parentOwner = parentOwner;
			r.parent = parent;

/*
			if(parent != null) {
				for(int i=0; i<parent.parentStamps.size(); i++) {
					r.parentStamps.add(parent.parentStamps.get(i));
					r.parentOwners.add(parent.parentOwners.get(i));
				}
			}

			r.parentStamps.add(new Integer(parentStamp));
			r.parentOwners.add(parentOwner);
*/
		}
		
		if (FAULT_TOLERANCE) {
		    if (parent != null && parent.reDone || parent == null && restarted) {
			r.reDone = true;
		    } 
		}
		
		q.addToHead(r);

		algorithm.jobAdded();

		if(SPAWN_DEBUG) {
			out.println("SATIN '" + ident.name() + 
				    "': Spawn, counter = " + r.spawnCounter.value +
				    ", stamp = " + r.stamp + ", parentStamp = " + r.parentStamp +
				    ", owner = " + r.owner + ", parentOwner = " + r.parentOwner);
		}
	}

	void handleActiveTuples() {
		String key = null;
		ActiveTuple data = null;

		while(true) {
			synchronized(this) {
				if(activeTupleKeyList.size() == 0) {
					gotActiveTuples = false;
					return;
				}

				// do upcall
				key = (String) activeTupleKeyList.remove(0);
				data = (ActiveTuple) activeTupleDataList.remove(0);
				if(TUPLE_DEBUG) {
					System.err.println("calling active tuple key = " + 
							   key + " data = " + data);
				}
			}

			try {
				data.handleTuple(key);
			} catch (Throwable t) {
				System.err.println("WARNING: active tuple threw exception: " + t);
			}
		}
	}

	synchronized void handleAborts() {
		int stamp;
		IbisIdentifier owner;

		while(true) {
			if(abortList.count > 0) {
				stamp = abortList.stamps[0];
				owner = abortList.owners[0];
				abortList.removeIndex(0);
			} else {
				gotAborts = false;
				return;
			}
			
			if(ABORT_DEBUG) {
				out.println("SATIN '" + ident.name() + ": handling abort message: stamp = " + 
					    stamp + ", owner = " + owner);
			}
			
			if(ABORT_STATS) {
				aborts++;
			}

			killChildrenOf(stamp, owner);

			if(ABORT_DEBUG) {
				out.println("SATIN '" + ident.name() + ": handling abort message: stamp = " + 
					    stamp + ", owner = " + owner + " DONE");
			}
		}
	}

	// Used for fault tolerance
	synchronized void handleAbortsAndStores() {
		int stamp;
		IbisIdentifier owner;

		while(true) {
			if(abortAndStoreList.count > 0) {
				stamp = abortAndStoreList.stamps[0];
				owner = abortAndStoreList.owners[0];
				abortAndStoreList.removeIndex(0);
			} else {
				gotAbortsAndStores = false;
				return;
			}
			
			
/*			if (NUM_CRASHES > 0) {		    
			    for (int i=1; i<NUM_CRASHES+1 && i<allIbises.size(); i++) {
				IbisIdentifier id = (IbisIdentifier) allIbises.get(i);
				if (id.equals(ident)) {
					return;
				}
			    }
			}*/

			
			
			killAndStoreChildrenOf(stamp, owner);

		}
	}



        // both here and in handleEmpty inlets: sendResult NOW if parentOwner is on remote machine
	void handleExceptions() {
		if(ASSERTS && !ABORTS) {
			System.err.println("cannot handle inlets, set ABORTS to true in Config.java");
			System.exit(1);
		}

		InvocationRecord r;
		while(true) {
			synchronized(this) {
				r = exceptionList.removeIndex(0);
				if (r == null) {
					gotExceptions = false;
					return;
				}
			}

			if(INLET_DEBUG) {
				out.println("SATIN '" + ident.name() + ": handling remote exception: " + r.eek + ", inv = " + r);
			}

			//  If there is an inlet, call it.
			handleInlet(r);

			r.spawnCounter.value--;
			if(ASSERTS && r.spawnCounter.value < 0) {
				out.println("Just made spawncounter < 0");
				new Exception().printStackTrace();
				System.exit(1);
			}
			if(INLET_DEBUG) {
				out.println("SATIN '" + ident.name() + ": handling remote exception DONE");
			}
		}
	}
	
	// The core of the fault tolerance mechanism, the crash recovery procedure
	synchronized void handleCrashes() {
	    if(CRASH_TIMING) {
		crashTimer.start();
	    }
	    if(COMM_DEBUG) {
		out.print("SATIN '" + ident.name() + ": handling crashes");
	    }

	    gotCrashes = false;
	    IbisIdentifier id = null;
	    while (crashedIbises.size() > 0) {
		id = (IbisIdentifier)crashedIbises.remove(0);
		if(COMM_DEBUG) {
		    out.println("SATIN '" + ident.name() + ": handling crash of " + id.name());
		}

		
		if (algorithm instanceof ClusterAwareRandomWorkStealing) {
		    ((ClusterAwareRandomWorkStealing) algorithm).checkAsyncVictimCrash(ident);
		}

		
		globalResultTable.removeReplica(id);
		
		if (id.equals(masterIdent)) {
		    //master has crashed, let's elect a new one
		    try {
			Registry r = ibis.registry();
			masterIdent = (IbisIdentifier) r.reelect("satin master", ident, id); //implement it!
			if (masterIdent.equals(ident)) {
			    master = true;
			}
			//barrier ports
			if (master) {
			    barrierReceivePort = 
				portType.createReceivePort("satin barrier receive port on " + ident);
			    barrierReceivePort.enableConnections();
			} else {
			    barrierSendPort.close();
			    barrierSendPort = portType.createSendPort("satin barrier send port on " + ident);
			    ReceivePortIdentifier barrierIdent = lookup("satin barrier receive port on " + masterIdent);
			    connect(barrierSendPort, barrierIdent);
			}
			
			//statistics
			if(stats && master) {
			    totalStats = new SatinStats();
			}
			
		    } catch (IOException e) {
			System.err.println("SATIN '" + ident.name() + "' :exception while electing a new master " + e.getMessage());
		    } catch (ClassNotFoundException e) {
			System.err.println("SATIN '" + ident.name() + "' :exception while electing a new master " + e.getMessage());
		    }
		    restarted = true;
		}
			 
		
		
/*		if (NUM_CRASHES > 0) {		    
		    for (int i=1; i<NUM_CRASHES+1 && i<allIbises.size(); i++) {
			IbisIdentifier id1 = (IbisIdentifier) allIbises.get(i);
			if (id1.equals(ident)) {
			    return;
			}
		    }
		}*/

		//abort all jobs stolen from id or descendants of jobs stolen from id
		killAndStoreSubtreeOf(id);
		
		//if using CRS, remove the asynchronously stolen job if it is owned by a
		//crashed machine
		if (algorithm instanceof ClusterAwareRandomWorkStealing) {
		    ((ClusterAwareRandomWorkStealing) algorithm).killOwnedBy(id);
		}

		//redo all jobs stolen by id (put them back in the task queue)
		redoStolenBy(id);
		
		//for debugging		
		crashedIbis = id;
		del = true;
	    }
	    if (CRASH_TIMING) {
		crashTimer.stop();
	    }
	    
	    if(COMM_DEBUG) {
		out.println("SATIN '" + ident.name() + ": handling crashes finished");	    
	    }
	    
	    notifyAll();
	}
	
	synchronized void handleDelete() {
	    	onStack.storeAll();
		killedOrphans += onStack.size();
		killedOrphans += q.size();
		printDetailedStats();
		
		//globalResultTable.exit();
	/*	try {
		    ibis.end();	
		} catch (IOException e) {
		    System.err.println("SATIN '" + ident.name() + "': unable to end ibis");
		}*/
		System.exit(0);
	}
	
	/** Used in fault tolerant Satin
	 *  If the job is being redone (redone flag is set to true)
	 *  perform a lookup in the global result table
	 * @param r invocation record of the job
	 * @return true if an entry was found, false otherwise
	 */
	protected boolean globalResultTableCheck(InvocationRecord r) { 
		if (TABLE_CHECK_TIMING) {
		    redoTimer.start();
		}
				

		    synchronized(this) {					
		    
			if (GRT_TIMING) {
			    lookupTimer.start();
			}
			
			Object key = null;
			if (GLOBALLY_UNIQUE_STAMPS && branchingFactor > 0) {
			    key = new Integer(r.stamp);
			} else {
			    key = r.getParameterRecord();
			}
			Object value = globalResultTable.lookup(key);
			if (GRT_TIMING) {					
			    lookupTimer.stop();
			}
			
			if (value == null) {
			    if (TABLE_CHECK_TIMING) {
				redoTimer.stop();
			    }
			    return false;
			}
			    
			
			if (GLOBAL_RESULT_TABLE_REPLICATED) {
			    
			    ReturnRecord rr = (ReturnRecord) value;
			    rr.assignTo(r);
			    r.spawnCounter.value--;
			    if (TABLE_CHECK_TIMING) {
				redoTimer.stop();
			    }
			    return true;
			    
			} else {
			    //distributed table
			    if (value instanceof IbisIdentifier) {
				//remote result
				
				SendPort s = getReplyPortNoWait( (IbisIdentifier) value);
				if (s == null) {
				    if (TABLE_CHECK_TIMING) {
					redoTimer.stop();
				    }				
				    return false;
				}
				//put the job in the stolen jobs list
				r.stealer = (IbisIdentifier) value;
				addToOutstandingJobList(r);				
				//send a request to the remote node
				try {
				    WriteMessage m = s.newMessage();
				    m.writeByte(Protocol.RESULT_REQUEST);
				    m.writeObject(key);
				    m.writeInt(r.stamp); //stamp and owner are not neccessary when using
				    m.writeObject(r.owner);//globally unique stamps, but let's not make things too complicated..
				    m.send();
				    m.finish();
				} catch (IOException e) {
				    System.err.println("SATIN '" + ident.name() +
					"': trying to send RESULT_REQUEST but got exception: " + e.getMessage());
				    outstandingJobs.remove(r);
				    return false;
				}
				if (TABLE_CHECK_TIMING) {
				    redoTimer.stop();
				}				
				return true;
				
			    } else {
				//local result, handle normally
				ReturnRecord rr = (ReturnRecord) value;
				rr.assignTo(r);
				r.spawnCounter.value--;
				if (TABLE_CHECK_TIMING) {
				    redoTimer.stop();
				}				
				return true;
			    }								
			}
		    } 
				
	}
	

	/**
	 * Waits for the jobs as specified by the spawncounter given, but
	 * meanwhile execute jobs from the end of the jobqueue (or rather,
	 * the head of the job queue, where new jobs are added).
	 * @param s the spawncounter.
	 */
	public void sync(SpawnCounter s) {
		InvocationRecord r;
		
//		System.err.println("SATIN " + ident.name() + ": sync() started");
		
		if(SPAWN_STATS) {
			syncs++;
		}

		if(s.value == 0) { // sync is poll
			satinPoll();
			handleDelayedMessages();
			return;
		}
		int numStealAttempts = 0;
		while(s.value > 0) {
//		    if(exiting) {
//			System.err.println("EXIT FROM SYNC");
//			exit();
//		    }

			if(SPAWN_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': Sync, counter = " + s.value);
			}

			satinPoll();
			handleDelayedMessages();

			r = q.getFromHead(); // Try the local queue
			if(r != null) {
				if (FAULT_TOLERANCE) {
				    if (r.reDone) {
					if(globalResultTableCheck(r)) {
				    	    continue;
					}
				    }
				}
				callSatinFunction(r);
			} else {
				algorithm.clientIteration();
			}
		}
		
//		System.err.println("SATIN " + ident.name() + ": sync() done");
	}

	/**
	 * Aborts the spawns that are the result of the specified invocation
	 * record. The invocation record of the invocation actually throwing
	 * the exception is also specified, but it is valid only for clones
	 * with inlets.
	 * @param outstandingSpawns parent of spawns that need to be aborted.
	 * @param exceptionThrower invocation throwing the exception.
	 */
	public synchronized void abort(InvocationRecord outstandingSpawns, InvocationRecord exceptionThrower) {
		// We do not need to set outstanding Jobs in the parent frame to null,
		// it is just used for assigning results.
		// get the lock, so no-one can steal jobs now, and no-one can change my tables.
		//		System.err.println("q " + q.size() + ", s " + onStack.size() + ", o " + outstandingJobs.size());
		try {
			if(ABORT_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': Abort, outstanding = " + outstandingSpawns + 
					    ", thrower = " + exceptionThrower);
			}
			InvocationRecord curr;

			if(SPAWN_STATS) {
				aborts++;
			}

			if(exceptionThrower != null) { // can be null if root does an abort.
				// kill all children of the parent of the thrower.
				if(ABORT_DEBUG) {
					out.println("killing children of " + exceptionThrower.parentStamp);
				}
				killChildrenOf(exceptionThrower.parentStamp, exceptionThrower.parentOwner);
			}
			
			// now kill mine
			if(outstandingSpawns != null) {
				int stamp;
				int me;
				if(outstandingSpawns.parent == null) {
					stamp = -1;
				} else {
					stamp = outstandingSpawns.parent.stamp;
				}

				if(ABORT_DEBUG) {
					out.println("killing children of my own: " + stamp);
				}
				killChildrenOf(stamp, ident);
			}

			if(ABORT_DEBUG) {
				out.println("SATIN '" + ident.name() + 
					    "': Abort DONE");
			}
		} catch (Exception e) {
			System.err.println("GOT EXCEPTION IN RTS!: " + e);
			e.printStackTrace();
		}
	}

	private void killChildrenOf(int targetStamp, IbisIdentifier targetOwner) {
		if(ABORT_TIMING) {
			abortTimer.start();
		}

		if(ASSERTS) {
			assertLocked(this);
		}
/*
  int iter = 0;
  while(true) {
  long abortCount = abortedJobs;

  System.err.println("killChildrenOf: iter = " + iter + " abort cnt = " + abortedJobs);
*/
		// try work queue, outstanding jobs and jobs on the stack
		// but try stack first, many jobs in q are children of stack jobs.
		onStack.killChildrenOf(targetStamp, targetOwner);
		q.killChildrenOf(targetStamp, targetOwner);
		outstandingJobs.killChildrenOf(targetStamp, targetOwner);
/*
  if(abortedJobs == abortCount) {
				// no more jobs were removed.
				break;
				}

				iter++;
				}
*/
		if(ABORT_TIMING) {
			abortTimer.stop();
		}
	}

	/* Used for fault tolerance */
	private void killAndStoreChildrenOf(int targetStamp, IbisIdentifier targetOwner) {

		if(ASSERTS) {
			assertLocked(this);
		}

		// try work queue, outstanding jobs and jobs on the stack
		// but try stack first, many jobs in q are children of stack jobs.
		onStack.killAndStoreChildrenOf(targetStamp, targetOwner);
		q.killChildrenOf(targetStamp, targetOwner);
		outstandingJobs.killAndStoreChildrenOf(targetStamp, targetOwner);

	}

	
	//abort every job that was spawned on targetOwner
	//or is a child of a job spawned on targetOwner
	//used for fault tolerance
	private void killSubtreeOf(IbisIdentifier targetOwner) {
		onStack.killSubtreeOf(targetOwner);
		q.killSubtreeOf(targetOwner);
		outstandingJobs.killSubtreeOf(targetOwner);
	}
	
	//used for fault tolerance
	private void killAndStoreSubtreeOf(IbisIdentifier targetOwner) {
		onStack.killAndStoreSubtreeOf(targetOwner);
		q.killSubtreeOf(targetOwner);
		outstandingJobs.killAndStoreSubtreeOf(targetOwner);
	}
	
	//used for fault tolerance
	private void redoStolenBy(IbisIdentifier targetOwner) {
	    outstandingJobs.redoStolenBy(targetOwner);
	}
		

	static boolean isDescendentOf(InvocationRecord child, int targetStamp, IbisIdentifier targetOwner) {
		if(child.parentStamp == targetStamp && child.parentOwner.equals(targetOwner)) {
			return true;
		}
		if(child.parent == null || child.parentStamp < 0) return false;

		return isDescendentOf(child.parent, targetStamp, targetOwner);
	}
	
	static boolean isDescendentOf1(InvocationRecord child, IbisIdentifier targetOwner) {
		if(child.parentOwner.equals(targetOwner)) {
			return true;
		}
		if(child.parent == null) return false;
		
		return isDescendentOf1(child.parent, targetOwner);
	}

/*
  static boolean isDescendentOf(InvocationRecord child, int targetStamp, IbisIdentifier targetOwner) {
  for(int i = 0; i< child.parentStamps.size(); i++) {
  int currStamp = ((Integer) child.parentStamps.get(i)).intValue();
  IbisIdentifier currOwner = (IbisIdentifier) child.parentOwners.get(i);

  if(currStamp == targetStamp && currOwner.equals(targetOwner)) {
  System.err.print("t");
  return true;
  }
  }
  return false;
  }
*/
	static boolean trylock(Object o) {
		try {
			o.notifyAll();
		} catch (IllegalMonitorStateException e) {
			return false;
		}

		return true;
	}

	static void assertLocked(Object o) {
		if(!trylock(o)) {
			System.err.println("AssertLocked failed!: ");
			new Exception().printStackTrace();
			System.exit(1);
		}
	}
	
	//connect upcall functions
	public boolean gotConnection(ReceivePort me, SendPortIdentifier applicant) {
//	    System.err.println("SATIN '" + ident.name() + "': got gotConnection upcall");
	    return true;
	}
	
	public void lostConnection(ReceivePort me, SendPortIdentifier johnDoe, Exception reason) {
	    if (COMM_DEBUG) {
		System.err.println("SATIN '" + ident.name() + "': got lostConnection upcall: " + johnDoe.ibis());
	    }
	    if (connectionUpcallsDisabled) return;
	    IbisIdentifier ident = johnDoe.ibis();
	    if (FAULT_TOLERANCE) {
		synchronized (this) {
		    crashedIbises.add(ident);
		    deadIbises.add(ident);
		    if (ident.equals(currentVictim)) {
			currentVictimCrashed = true;
		    }				
		    gotCrashes = true;
		    Victim v = victims.remove(ident);
		    notifyAll();
		    if (v != null && v.s != null) {
			try {
			    v.s.close();
			} catch (IOException e) {
			    System.err.println("port.free() throws exception " + e.getMessage());
			}
		    }
		    
		}
		
		
	    }
	}

	public void lostConnection(SendPort me, ReceivePortIdentifier johnDoe, Exception reason) {
	    if (COMM_DEBUG) {
		System.err.println("SATIN '" + ident.name() + "': got SENDPORT lostConnection upcall: " + johnDoe.ibis());
	    }
	    if (connectionUpcallsDisabled) return;	
	    IbisIdentifier ident = johnDoe.ibis();
	    if (FAULT_TOLERANCE) {
		synchronized (this) {
		    crashedIbises.add(ident);
		    deadIbises.add(ident);
		    if (ident.equals(currentVictim)) {
			currentVictimCrashed = true;
		    }
		    
		    gotCrashes = true;
		    Victim v = victims.remove(ident);
		    notifyAll();
		    if (v != null && v.s != null) {
			try {
			    v.s.close();
			} catch (IOException e) {
			    System.err.println("port.free() throws exception " + e.getMessage());
			}
		    }
		
		}
	    }
	    
	    
	}



        /* ------------------- tuple space stuff ---------------------- */

	protected void broadcastTuple(String key, Serializable data) {	
		long count = 0;
		int size = 0;


		if(TUPLE_DEBUG) {
			System.err.println("SATIN '" + ident.name() + 
					   "': bcasting tuple " + key);
		}
		
		synchronized (this) {
		    size = victims.size();
		}

		if(! SUPPORT_TUPLE_MULTICAST && size == 0) return; // don't multicast when there is no-one.
		if(size == 0 && sequencer == null) return;

		if(TUPLE_TIMING) {
			tupleTimer.start();
		}

		if(SUPPORT_TUPLE_MULTICAST) {
			int seqno = 0;
			try {
				WriteMessage writeMessage = tuplePort.newMessage();
				writeMessage.writeByte(Protocol.TUPLE_ADD);
				if (sequencer != null) {
				    tupleOrderingSeqTimer.start();
				    seqno = sequencer.getSeqno("TupleSpace");
				    tupleOrderingSeqTimer.stop();
				    writeMessage.writeInt(seqno);
				}
				writeMessage.writeObject(key);
				writeMessage.writeObject(data);

				if(TUPLE_STATS) {
					tupleMsgs++;
					count = writeMessage.finish();
				} else {
					writeMessage.finish();
				}

			} catch (IOException e) {
				if (!FAULT_TOLERANCE) {
				    System.err.println("SATIN '" + ident.name() + 
						   "': Got Exception while sending tuple update: " + e);
				    e.printStackTrace();
				    System.exit(1);
				}
				//always happens after crash
			}
			if (sequencer != null) {
			    synchronized(tuplePort) {
				while (expected_seqno <= seqno) {
				    try {
					tuplePort.wait();
				    } catch (Exception e) {
				    }
				}
			    }
			}
			if (sequencer != null) {
			    synchronized(tuplePort) {
				while (expected_seqno <= seqno) {
				    try {
					tuplePort.wait();
				    } catch(Exception e) {
				    }
				}
			    }
			}
		} else {
			for(int i=0; i<size; i++) {
				try {
				    SendPort s = null;
				    synchronized(this) {
					s = victims.getPort(i);
				    }
					WriteMessage writeMessage = s.newMessage();
					writeMessage.writeByte(Protocol.TUPLE_ADD);
					writeMessage.writeObject(key);
					writeMessage.writeObject(data);

					if(TUPLE_STATS && i == 0) {
						tupleMsgs++;
						count = writeMessage.finish();
					}
					else {
						writeMessage.finish();
					}

				} catch (IOException e) {
					System.err.println("SATIN '" + ident.name() + 
							   "': Got Exception while sending tuple update: " + e);
					System.exit(1);
				}
			}
		}

		tupleBytes += count;

		if(TUPLE_TIMING) {
			tupleTimer.stop();
//			System.err.println("SATIN '" + ident.name() + ": bcast of " + count + " bytes took: " + tupleTimer.lastTime());
		}
	}

	protected void broadcastRemoveTuple(String key) {
		long count = 0;
		int size = 0;
		
		if(TUPLE_DEBUG) {
			System.err.println("SATIN '" + ident.name() + 
					   "': bcasting remove tuple" + key);
		}
		
		synchronized (this) {
		    size = victims.size();
		}

		if(size == 0) return; // don't multicast when there is no-one.
		if(size == 0 && sequencer == null) return;

		if(TUPLE_TIMING) {
			tupleTimer.start();
		}

		if(SUPPORT_TUPLE_MULTICAST) {
			int seqno = 0;
			try {
				WriteMessage writeMessage = tuplePort.newMessage();
				writeMessage.writeByte(Protocol.TUPLE_DEL);
				if (sequencer != null) {
				    seqno = sequencer.getSeqno("TupleSpace");
				    writeMessage.writeInt(seqno);
				}
				writeMessage.writeObject(key);

				if(TUPLE_STATS) {
					tupleMsgs++;
					count += writeMessage.finish();
				}
				else {
					writeMessage.finish();
				}

			} catch (IOException e) {
				if (!FAULT_TOLERANCE) {
				    System.err.println("SATIN '" + ident.name() + 
						   "': Got Exception while sending tuple update: " + e);
				    System.exit(1);
				}
				//always happen after crashes
			}
			if (sequencer != null) {
			    synchronized(tuplePort) {
				while (expected_seqno <= seqno) {
				    try {
					tuplePort.wait();
				    } catch (Exception e) {
				    }
				}
			    }
			}
			if (sequencer != null) {
			    synchronized(tuplePort) {
				while (expected_seqno <= seqno) {
				    try {
					tuplePort.wait();
				    } catch(Exception e) {
				    }
				}
			    }
			}
		} else {
			for(int i=0; i<size; i++) {
				try {
					SendPort s = victims.getPort(i);
					WriteMessage writeMessage = s.newMessage();
					writeMessage.writeByte(Protocol.TUPLE_DEL);
					writeMessage.writeObject(key);

					if(TUPLE_STATS && i == 0) {
						tupleMsgs++;
						count += writeMessage.finish();
					}
					else {
						writeMessage.finish();
					}

				} catch (IOException e) {
					System.err.println("SATIN '" + ident.name() + 
							   "': Got Exception while sending tuple update: " + e);
					System.exit(1);
				}
			}
		}

		tupleBytes += count;

		if(TUPLE_TIMING) {
			tupleTimer.stop();
//			System.err.println("SATIN '" + ident.name() + ": bcast of " + count + " bytes took: " + tupleTimer.lastTime());
		}
	}

	/**
	 * Returns the current Satin instance.
	 * @return the current Satin instance.
	 */
	public static Satin getSatin() {
	    return this_satin;
	}

	private SatinStats createStats() {
		SatinStats s = new SatinStats();

		s.spawns = spawns;
		s.jobsExecuted = jobsExecuted;
		s.syncs = syncs;
		s.aborts = aborts;
		s.abortMessages = abortMessages;
		s.abortedJobs = abortedJobs;

		s.stealAttempts = stealAttempts;
		s.stealSuccess = stealSuccess;
		s.tupleMsgs = tupleMsgs;
		s.tupleBytes = tupleBytes;
		s.stolenJobs = stolenJobs;
		s.stealRequests = stealRequests;
		s.interClusterMessages = interClusterMessages;
		s.intraClusterMessages = intraClusterMessages;
		s.interClusterBytes = interClusterBytes;
		s.intraClusterBytes = intraClusterBytes;

		s.stealTime = stealTimer.totalTimeVal();
		s.handleStealTime = handleStealTimer.totalTimeVal();
		s.abortTime = abortTimer.totalTimeVal();
		s.idleTime = idleTimer.totalTimeVal();
		s.idleCount = idleTimer.nrTimes();
		s.pollTime = pollTimer.totalTimeVal();
		s.pollCount = pollTimer.nrTimes();
		s.tupleTime = tupleTimer.totalTimeVal();
		s.tupleWaitTime = tupleOrderingWaitTimer.totalTimeVal();
		s.tupleWaitCount = tupleOrderingWaitTimer.nrTimes();
		s.tupleSeqTime = tupleOrderingSeqTimer.totalTimeVal();
		s.tupleSeqCount = tupleOrderingSeqTimer.nrTimes();

		s.invocationRecordWriteTime = invocationRecordWriteTimer.totalTimeVal();
		s.invocationRecordWriteCount = invocationRecordWriteTimer.nrTimes();
		s.invocationRecordReadTime = invocationRecordReadTimer.totalTimeVal();
		s.invocationRecordReadCount = invocationRecordReadTimer.nrTimes();
		
		//fault tolerance
		if (FAULT_TOLERANCE) {
		    s.tableUpdates = globalResultTable.numUpdates;
		    //each remote lookup == 2 lookups in table (local & remote) -- must clean up this code later
		    s.tableLookups = globalResultTable.numLookups - globalResultTable.numRemoteLookups;		
		    s.tableSuccessfulLookups = globalResultTable.numLookupsSucceded - globalResultTable.numRemoteLookups;
		    s.tableRemoteLookups = globalResultTable.numRemoteLookups;
		    s.killedOrphans = killedOrphans;
		
		    s.tableLookupTime = lookupTimer.totalTimeVal();
		    s.tableUpdateTime = updateTimer.totalTimeVal();
		    s.tableHandleUpdateTime = handleUpdateTimer.totalTimeVal();
		    s.tableHandleLookupTime = handleLookupTimer.totalTimeVal();
		    s.crashHandlingTime = crashTimer.totalTimeVal();
		    s.addReplicaTime = addReplicaTimer.totalTimeVal();
		}

		return s;
	}

        /* ------------------- pause/resume stuff ---------------------- */

	/** Pause Satin operation. 
	    This method can optionally be called before a large sequential part in a program.
	    This will temporarily pause Satin's internal load distribution strategies to 
	    avoid communication overhead during sequential code.
	**/
	static void pause() {
		if(this_satin == null || !this_satin.upcalls) return;
		this_satin.receivePort.disableUpcalls();
	}

	/** Resume Satin operation. 
	    This method can optionally be called after a large sequential part in a program.
	**/
	static void resume() {
		if(this_satin == null || !this_satin.upcalls) return;
		this_satin.receivePort.enableUpcalls();
	}

	/** Returns whether it might be useful to spawn more methods.
	    If there is enough work in the system to keep all processors busy, this
	    method returns false.
	**/
	static boolean needMoreJobs() {
		// This can happen in sequential programs.
		if(this_satin == null) {
			return false;
		}
		synchronized(this_satin) {
			int size = this_satin.victims.size();
			// if(size == 1 && this_satin.closed) return false; // No need to spawn work on one machine.
			// No no, size == 1 means that there is one OTHER
			// machine ... (Ceriel)
			if(size == 0 && this_satin.closed) return false; // No need to spawn work on one machine.
		
			if(this_satin.q.size() / (size+1) > this_satin.suggestedQueueSize) return false;
		}

		return true;
	}

	/** Returns whether the current method was generated by the machine it is running on.
	    methods can be distributed to remote machines by the Satin runtime system,
	    in which case this method returns false.
	 **/
	static boolean localJob() {
	    if(this_satin == null) return true; // sequential run

	    if(this_satin.parentOwner == null) return true; // root job

		return this_satin.parentOwner.equals(this_satin.ident);
	}
}
